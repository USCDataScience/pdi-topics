{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGU Topic Modeling\n",
    "\n",
    "This notebook does topic modeling on EGU abstracts from 2009 to 2018, the main premise is that the PDF files are organized in the same way, first the EGU copyright notice, then the abstract title, then the authors and lastly the abstract content.\n",
    "\n",
    "We parsed the PDFs using PDFMiner's utility pdf2txt\n",
    "\n",
    "```sh\n",
    "ls *.pdf | xargs -n1 -P8 bash -c 'pdf2txt.py -o output/$0.txt -t text $0'\n",
    "```\n",
    "\n",
    "The current notebook uses Solr to index the parsed content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Schema:**\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "       'year': year,\n",
    "       'file': fname,\n",
    "       'title' : title,\n",
    "       'entities': entities,\n",
    "       'abstract': abstract,\n",
    "       'category': category,\n",
    "       'sessions': full_session_code,\n",
    "       'presentation': presentation,\n",
    "       'timestamp': datetime.now().isoformat(),\n",
    "      }\n",
    "```\n",
    "\n",
    "* category: the main session id, CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* presentation: oral, poster, pico etc.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Querying Solr\n",
    "\n",
    "# The following parameters have to be in sync with the ScatterText parameters, if we want to compare 2 years then we need to\n",
    "# use the same years in the ScatterText cell. In this case we are comparing 2 sessions for all the years. \n",
    "\n",
    "terms = ['ice', 'climate']\n",
    "years = ['*']\n",
    "entities = ['*']\n",
    "sessions = ['CR','NH']\n",
    "\n",
    "# We sample Solr for up to 1000 documents that comply with our criteria\n",
    "page_size = 1000\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "terms_query = '%20OR%20abstract:'.join('*' + terms + '*') \n",
    "years_query = '%20OR%20year:'.join(years)  \n",
    "entities_query = '%20OR%20entities:'.join(entities)\n",
    "sessions_query = '%20OR%20category:'.join(sessions)\n",
    "\n",
    "while (more_results):    \n",
    "    solr_query = 'q=(abstract:{})%20AND%20(year:{})%20AND%20(entities:{})%20AND%20(category:{})&wt=json&rows={}&cursorMark={}&sort=id+asc'.format(terms_query,\n",
    "                                                                                        years_query,\n",
    "                                                                                        entities_query,\n",
    "                                                                                        sessions_query,\n",
    "                                                                                        page_size,\n",
    "                                                                                        cursorMark)\n",
    "    solr_url = solr_root + solr_query\n",
    "    print('Querying: \\n' + solr_url)\n",
    "    req = urllib.request.Request(solr_url)\n",
    "    # parsing response\n",
    "    r = urllib.request.urlopen(req).read()\n",
    "    json_response = json.loads(r.decode('utf-8'))\n",
    "    solr_documents.extend(json_response['response']['docs'])\n",
    "    nextCursorMark = json_response['nextCursorMark']\n",
    "    if (nextCursorMark == cursorMark):\n",
    "        more_results = False\n",
    "        break\n",
    "    else: \n",
    "        cursorMark = nextCursorMark\n",
    "\n",
    "print('Processing {} documents'.format(len(solr_documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Cleaning our documents \n",
    "\n",
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words (used for Gensim only)\n",
    "my_stop_words = {\n",
    "                    'area', 'data', 'event', 'doc', 'group', 'research', \n",
    "                    'metadata', 'content', 'sharing', 'previous', 'http', \n",
    "                    '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010',\n",
    "                    '2011', '2012', '2013', '2014','2015', '2016', '2017',\n",
    "                }\n",
    "\n",
    "stop_words = my_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "scattertext_documents = []\n",
    "gensim_documents = []\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    gensim_documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model using Gensim a library for topic modeling, the output is a list of topics present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Using GENSIM to do topic modelling, this cell takes some time... hang on.\n",
    "\n",
    "num_passes = 10 #num pases should be adjusted, 5 is just a guesstimate of when convergence will be achieved.\n",
    "num_topics = 20\n",
    "words_per_topic = 7\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_documents)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in gensim_documents]\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScatterText\n",
    "* Now we're going to use the ScatterText library to visualize some binary categories token distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: loading our documents into a Panda dataframe for ScatterText and listing the document distributions\n",
    "\n",
    "df = pd.DataFrame.from_dict(scattertext_documents)\n",
    "axis_year = pd.DataFrame(df.groupby('year')['year'].count())\n",
    "axis_session = pd.DataFrame(df.groupby('session')['session'].count())\n",
    "print(axis_session, axis_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a trained model we can classify a new unseen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Classifying an unseen document using our GENSIM model\n",
    "\n",
    "# For practical purposes we use a mocked up document but we can easily query Solr or another store to get the content we want to classify\n",
    "# Eventually all this should be served in as a web service \n",
    "#taken from https://meetingorganizer.copernicus.org/EGU2018/EGU2014-2415.pdf\n",
    "\n",
    "unseen_document = \"\"\"\n",
    "Waves  in  the  Southern  Ocean  are  the  largest  in  the  planet.  In  the  Southern  Hemisphere,  the  absence  of  large\n",
    "landmasses at high latitudes allows the wind to feed energy into the ocean over a virtually unlimited fetch. The\n",
    "enormous amount air-sea momentum exchanged over the Southern Ocean plays a substantial role on the global\n",
    "climate. However, large biases affect the estimation of wave regime around the Antarctic continent making climate\n",
    "prediction susceptible to uncertainty.\n",
    " \"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(unseen_document))\n",
    "predicted_topics = lda_model[vec]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our model with PyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Using pyLDAvis to visualize our topic distributions in the principal component axis.\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus, topic_model=lda_model, dictionary=dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "> Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017. Link to preprint: arxiv.org/abs/1703.00565"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
