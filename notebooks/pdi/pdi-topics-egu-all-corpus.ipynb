{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGU Topic Modeling and prediction\n",
    "\n",
    "This notebook uses topic modeling to analyze the EGU conference using the abstracts submitted for the years 2011 to 2018. The notebook can be used to detect trends and visualize topics along these years using one or many categories. The abstracts were parsed into text and ingested into a Apache Solr instance.\n",
    "\n",
    "We parsed the PDFs using PDFMiner's utility pdf2txt\n",
    "\n",
    "```sh\n",
    "ls *.pdf | xargs -n1 -P8 bash -c 'pdf2txt.py -o output/$0.txt -t text $0'\n",
    "```\n",
    "\n",
    "**Solr Schema**\n",
    " \n",
    " \n",
    " \n",
    "```json\n",
    "doc = {\n",
    "       'year': year,\n",
    "       'file': fname,\n",
    "       'title' : title,\n",
    "       'entities': entities,\n",
    "       'abstract': abstract,\n",
    "       'category': category,\n",
    "       'sessions': full_session_code,\n",
    "       'presentation': presentation,\n",
    "       'timestamp': datetime.now().isoformat(),\n",
    "      }\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "\"entities\":[\n",
    "    \"Jeffrey Obelcz  and Warren T. Wood\",\n",
    "    \"NRC Postdoctoral Fellow\",\n",
    "    \"Naval Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States jbobelcz@gmail.com\",\n",
    "    \"Naval\",\n",
    "    \"Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States\"],\n",
    "\"id\": \"EGU2018-9778\",\n",
    "\"sessions\": [\"ESSI4.3\"],\n",
    "\"file\": [\"EGU2018-9778\"],\n",
    "\"presentation\": [\"Posters\"],\n",
    "\"year\": [2018],\n",
    "\"title\": [\"Towards a Quantitative Understanding of Parameters Driving Submarine Slope Failure: A Machine Learning Approach\"],\n",
    "\"category\": [\"ESSI\"],\n",
    "\"abstract\":[\"Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales ...\"]\n",
    "}\n",
    "```\n",
    "\n",
    "## Application\n",
    "\n",
    "This notebook can be used to analyze what a corpora of scientific text talks about, in this case we used EGU abstracts but it can be used on any text corpora.\n",
    "\n",
    "The topics are examined using LDAVis, which displays the topics on an X-Y plot (intertopic distance map). Topic are represented by circles whose areas are proportional to the relative prevalences of the topics in the corpus. In the display the user can enter a topic number (note 1-base numbering vs. 0-base numbering in Topic List below cells 5 and 7); the terms are displayed on the right, ranked by significance (weight). A topic can be selected on the fly by hovering over its circle; clicking selects that topic. A user can also click on a term in the RH panel to show the topics in which that term occurs. \n",
    "\n",
    "The slider at the top of the RH panel allows the user to vary the “saliency”, i.e. uniqueness of the terms to that topic. A value of 0.6 is optimal, according to the authors of the algorithm. Blue bars represent overall term frequency while red bars show term frequency within the topic, which will be different when the saliency Is selected be less than one.\n",
    "\n",
    "Please see the annotated image of the LDAVis display.\n",
    "\n",
    "![](https://raw.githubusercontent.com/USCDataScience/pdi-topics/master/notebooks/pdi/img/pylda.png)\n",
    "\n",
    "\n",
    "\n",
    "> The final cell of this notebook lists the titles and IDs of abstracts belonging to a specified topic.\n",
    "* category: the Program Group, e.g. CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* presentation: Presentation type: oral, poster, pico etc.\n",
    "* session: the session that the abstract appeared in; sessions may be co-organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "EGU Programme Groups:\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)\n",
    "\n",
    "### Interdisciplinary Events (IE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Querying Solr\n",
    "\n",
    "# terms = ['ice', 'climate'] to include only abstracts with specified terms\n",
    "terms = ['*']\n",
    "years = ['*']\n",
    "entities = ['*']\n",
    "sessions = ['US','IE','GDB','ML','PC']\n",
    "\n",
    "# Return \"page_size\" documents with each Solr query until complete\n",
    "page_size = 1000\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "\n",
    "if terms[0] != '*':\n",
    "    terms_wirldcard = ['*' + t + '*' for t in terms]\n",
    "else:\n",
    "    terms_wirldcard = ['*']\n",
    "    \n",
    "if sessions[0] != '*':\n",
    "    sessions_wirldcard = ['*' + s + '*' for s in sessions]\n",
    "else:\n",
    "    sessions_wirldcard = ['*']\n",
    "    \n",
    "if entities[0] != '*':\n",
    "    entities_wirldcard = ['*' + e + '*' for e in entities]\n",
    "else:\n",
    "    entities_wirldcard = ['*']\n",
    "\n",
    "terms_query = '%20OR%20abstract:'.join(terms_wirldcard)\n",
    "years_query = '%20OR%20year:'.join(years)  \n",
    "entities_query = '%20OR%20entities:'.join(entities_wirldcard)\n",
    "sessions_query = '%20OR%20sessions:'.join(sessions_wirldcard)\n",
    "query_string = 'q=(abstract:{}%20AND%20abstract:/.{{2}}.*/%20AND%20NOT%20title:/.{{300}}.*/)%20AND%20(year:{})' + \\\n",
    "                '%20AND%20(entities:{})%20AND%20(sessions:{})&wt=json&rows={}&cursorMark={}&sort=id+asc'\n",
    "while (more_results):    \n",
    "    solr_query = query_string.format(terms_query,\n",
    "                                     years_query,\n",
    "                                     entities_query,\n",
    "                                     sessions_query,\n",
    "                                     page_size,\n",
    "                                     cursorMark)\n",
    "    solr_url = solr_root + solr_query\n",
    "    print('Querying: \\n' + solr_url)\n",
    "    req = urllib.request.Request(solr_url)\n",
    "    # parsing response\n",
    "    r = urllib.request.urlopen(req).read()\n",
    "    json_response = json.loads(r.decode('utf-8'))\n",
    "    solr_documents.extend(json_response['response']['docs'])\n",
    "    nextCursorMark = json_response['nextCursorMark']\n",
    "    if (nextCursorMark == cursorMark):\n",
    "        more_results = False\n",
    "        break\n",
    "    else: \n",
    "        cursorMark = nextCursorMark\n",
    "\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Cleaning our documents \n",
    "\n",
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words (used for Gensim only)\n",
    "my_stop_words = {\n",
    "                    'area', 'data', 'event', 'doc', 'group', 'research', 'model',\n",
    "                    'metadata', 'content', 'sharing', 'previous', 'http','study'\n",
    "                }\n",
    "\n",
    "years = [str(year) for year in range(2000,2020)]\n",
    "words_and_years = my_stop_words.union(years)\n",
    "stop_words = words_and_years.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    # We also remove unicode characters\n",
    "    document = [i.encode('ascii',errors='ignore').decode() for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "gensim_documents = []\n",
    "#\n",
    "document_list = []\n",
    "\n",
    "# artifact of parsing the sessions from the pdf documents\n",
    "garbage_str = '</a>'\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    if 'sessions' in doc:\n",
    "        sindex = doc['sessions'][0].find(garbage_str)\n",
    "        if sindex != -1:\n",
    "            sessions = doc['sessions'][0][0:sindex]\n",
    "        else: \n",
    "            sessions = doc['sessions'][0]\n",
    "    else:\n",
    "        sessions = 'NAN'\n",
    "    document_list.append({ 'id': doc['id'],\n",
    "                                   'text': ' '.join(document), \n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'category': doc['category'][0].replace('/', '').replace('<', ''),\n",
    "                                   'sessions':sessions})\n",
    "    \n",
    "    gensim_documents.append(document)\n",
    "\n",
    "df = pd.DataFrame.from_dict(document_list)\n",
    "axis_category = pd.DataFrame(df.groupby(['category', 'year'])['category'].count()).rename(columns={'category': 'count'})\n",
    "print(axis_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model using Gensim a library for topic modeling, the output is a list of topics present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Using GENSIM to do topic modelling, this cell takes some time... hang on.\n",
    "\n",
    "num_passes = 3 #num pases should be adjusted, 5 is just a guesstimate of when convergence will be achieved.\n",
    "num_topics = 13\n",
    "words_per_topic = 7\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_documents)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in gensim_documents]\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "\n",
    "# now list the topics and words with probabilities\n",
    "for topic in topics:\n",
    "    t = str((int(topic[0])+ 1))\n",
    "    print('Topic ' + t + ': ', topic[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a trained model we can classify a new unseen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Classifying an unseen document using our GENSIM model\n",
    "\n",
    "# For practical purposes we use a mocked up document but we can easily query Solr or another store to get the content we want to classify\n",
    "# Eventually all this should be served in as a web service \n",
    "# taken from https://meetingorganizer.copernicus.org/EGU2018/EGU2014-2415.pdf\n",
    "\n",
    "unseen_document = \"\"\"\n",
    "Waves  in  the  Southern  Ocean  are  the  largest  in  the  planet.  In  the  Southern  Hemisphere,  the  absence  of  large\n",
    "landmasses at high latitudes allows the wind to feed energy into the ocean over a virtually unlimited fetch. The\n",
    "enormous amount air-sea momentum exchanged over the Southern Ocean plays a substantial role on the global\n",
    "climate. However, large biases affect the estimation of wave regime around the Antarctic continent making climate\n",
    "prediction susceptible to uncertainty.\n",
    " \"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(unseen_document))\n",
    "predicted_topics = lda_model[vec]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our model with PyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Using pyLDAvis to visualize our topic distributions in the principal component axis.\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus,\n",
    "                        topic_model=lda_model,\n",
    "                        dictionary=dictionary,\n",
    "                        lambda_step=0.6,\n",
    "                        sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: We create a matrix of topics and documents and \n",
    "# we print a list of the documents that belong to a particular topic.\n",
    "topic_number = 1\n",
    "minum_likelihood = 0.01\n",
    "\n",
    "# We create a list for each topic containing the terms and an empty document list\n",
    "topic_list = [{'topic':t[0],'terms': t[1], 'documents': []} for t in topics]\n",
    "\n",
    "# Add each document to their predicted topics if the probability is above the minimum_likelihood\n",
    "for i in range(len(solr_documents)):\n",
    "    doc_prob = lda_model.get_document_topics(bow=lda_corpus[i], minimum_probability=minum_likelihood)\n",
    "    doc_title = solr_documents[i]['title'][0]\n",
    "    doc_id = solr_documents[i]['id']\n",
    "    doc_sessions = solr_documents[i]['sessions']\n",
    "    # Each document could contain more than one topic, we traverse them and add the url to n topics\n",
    "    for prob in doc_prob:\n",
    "        topic_index = prob[0]\n",
    "        topic_probability = prob[1]\n",
    "        topic_list[topic_index]['documents'].append({'id':doc_id, \n",
    "                                                     'title': doc_title, \n",
    "                                                     'probability': topic_probability})\n",
    "\n",
    "topic_members = topic_list[topic_number]['documents']\n",
    "topic_members = sorted(topic_members, key=lambda k: k['probability'], reverse=True)\n",
    "\n",
    "# print(topic_members)\n",
    "print ('{} documents in this topic {} > {}\\n'.format(len(topic_members), topic_number, topic_list[topic_number]['terms']))\n",
    "for doc in topic_members:\n",
    "    print('\\n')\n",
    "    print(doc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
