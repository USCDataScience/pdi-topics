{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polar Data Insights and Search Analytics for the Deep and Scientific Web \n",
    "\n",
    "The pitch - \n",
    "\n",
    "\"Our preliminary work in this area has shown that the unstructured textual data, when combined with structured scientific information can inform answers to grand challenge problems such as identifying ice sheet breakage/melt over decadal time spans; bird migration around Greenland, oil spills and natural disasters, sea ice decline and its relation to natural disasters, and other critical questions for the Polar community derived from the Presidentâ€™s National Strategy for the Arctic Region.\"\n",
    "\n",
    "\n",
    "## Polar Data Insights (PDI for short)\n",
    "\n",
    "![stack](stack.png)\n",
    "\n",
    "\n",
    "## Focused Crawls using BCube Nutch's fork\n",
    "\n",
    "\n",
    "\n",
    "## Solr schema (relevant fields)\n",
    "\n",
    "\n",
    "```xml\n",
    "\n",
    " <field name=\"_version_\" type=\"long\" indexed=\"false\" stored=\"true\"/>\n",
    "    <field name=\"id\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n",
    "    <!-- fields for index-basic plugin -->\n",
    "    <field name=\"host\" type=\"url\" stored=\"true\" indexed=\"true\"/>\n",
    "    <field name=\"url\" type=\"url\" stored=\"true\" indexed=\"true\" required=\"true\"/>\n",
    "    <!-- stored=true for highlighting, use term vectors  and positions for fast highlighting -->\n",
    "    <field name=\"content\" type=\"text_general\" stored=\"true\" indexed=\"false\"/>\n",
    "    <field name=\"title\" type=\"text_general\" stored=\"true\" indexed=\"true\"/>\n",
    "    <field name=\"tstamp\" type=\"date\" stored=\"true\" indexed=\"false\"/>\n",
    "    <!-- catch-all field -->\n",
    "    <field name=\"text\" type=\"text_general\" stored=\"false\" indexed=\"true\" multiValued=\"true\"/>\n",
    "    <!-- fields for index-anchor plugin -->\n",
    "    <field name=\"anchor\" type=\"text_general\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n",
    "    <!-- fields for index-more plugin -->\n",
    "    <field name=\"type\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n",
    "    <field name=\"contentLength\" type=\"string\" stored=\"true\" indexed=\"false\"/>\n",
    "    <field name=\"lastModified\" type=\"date\" stored=\"true\" indexed=\"false\"/>\n",
    "    <field name=\"date\" type=\"tdate\" stored=\"true\" indexed=\"true\"/>\n",
    "    <!-- fields for languageidentifier plugin -->\n",
    "    <field name=\"lang\" type=\"string\" stored=\"true\" indexed=\"true\"/>\n",
    "\n",
    "    <!-- - - - - - - BCUBE PLUGINS - - - - - -  -->\n",
    "\n",
    "    <!-- fields for index-rawcontent plugin -->\n",
    "    <field name=\"raw_content\" type=\"text_general\" stored=\"true\" indexed=\"true\" multiValued=\"false\"/>\n",
    "    <!-- field for index-xmlnamespaces plugin -->\n",
    "    <field name=\"xml_namespaces\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n",
    "    <!-- field for index-links plugin -->\n",
    "    <field name=\"inlinks\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n",
    "    <field name=\"outlinks\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n",
    "    <!-- field for index-bcubefilter plugin -->\n",
    "    <field name=\"url_hash\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"false\"/>\n",
    "    <!-- field for index-httpresponse plugin -->\n",
    "    <field name=\"response_headers\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\"/>\n",
    "</field>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download nltk's wordnet first\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import json\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be customized to use any visible solr endpoint\n",
    "# For convenience on the case of web crawls we use the content, url and mime type fields\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/rda/select?'\n",
    "solr_fields = 'fl=url,content,type&'\n",
    "solr_query = 'q=url:\"case-statement\"&rows=200&start=1&wt=json'\n",
    "solr_url = solr_root + solr_fields + solr_query\n",
    "req = urllib.request.Request(solr_url)\n",
    "# parsing response\n",
    "r = urllib.request.urlopen(req).read()\n",
    "json_response = json.loads(r.decode('utf-8'))\n",
    "solr_documents = json_response['response']['docs']\n",
    "print(\"Processing {0} documents. \\n\".format(len(solr_documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Now we tokenize each document and remove stop words and apply stemming (Wordnet lemmatizer)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words\n",
    "my_stop_words = {\n",
    "                    'http', 'www', 'edu', 'org', 'com', 'rda', 'data', 'researcher', 'event', 'service',\n",
    "                    'group', 'research', 'community', 'use', 'work', 'member', 'case', 'working', 'science',\n",
    "                    'meeting', 'organisational', 'news', 'plenary', 'recommendation', 'project', 'standard',\n",
    "                    'statement', 'school', 'university', 'membership', 'output', '2017', 'brokering',\n",
    "                    'stakeholder', 'repository', 'user', 'citation', 'chair', 'framework', 'information',\n",
    "                    'metadata', 'content', 'sharing', 'pid'\n",
    "                }\n",
    "stop_words = my_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "document_list = []\n",
    "# pairs is a list of the urls and the size of their content\n",
    "pairs = []\n",
    "# just the documents urls\n",
    "urls = []\n",
    "\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['content'][0])\n",
    "    document_list.append(document)\n",
    "    pairs.append((doc['url'],len(document)))\n",
    "    urls.append(doc['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Building the LDA model using Gensim a library for topic modeling, the output is a list of topics present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_passes = 5\n",
    "num_topics = 20\n",
    "words_per_topic = 6\n",
    "\n",
    "dictionary = corpora.Dictionary(document_list)\n",
    "corpus = [dictionary.doc2bow(text) for text in document_list]\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "# Now let's print the topics found\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following code builds a list of all the documents that belong to a particular topic and their calculated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minum_likelihood = 0.1 # 10%\n",
    "total_documents = len(urls)\n",
    "\n",
    "# We create a list for each topic containing the terms and an empty document list\n",
    "topic_list = [{'topic':t[0],'terms': t[1], 'documents': []} for t in topics]\n",
    "# Add each document to their predicted topics if the probability is above the minimum_likelihood\n",
    "for i in range(total_documents):\n",
    "    doc_prob = lda_model.get_document_topics(bow=corpus[i],minimum_probability=minum_likelihood)\n",
    "    document_url = urls[i]\n",
    "    # Each document could contain more than one topic, we traverse them and add the url to n topics\n",
    "    for prob in doc_prob:\n",
    "        topic_index = prob[0]\n",
    "        topic_probability = prob[1]\n",
    "        topic_list[topic_index]['documents'].append((document_url,topic_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can select a topic and then we'll print all the documents for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_of_interest = 11 # the topic index\n",
    "def getkey(doc):\n",
    "    return doc[1]\n",
    "\n",
    "print(\"Documents in Topic {0} ({1})\".format(topic_of_interest,topic_list[topic_of_interest]['terms']))\n",
    "for doc in sorted(topic_list[topic_of_interest]['documents'],key=getkey):\n",
    "    print(\" Document: {0} \\n - Probability:{1}\".format(doc[0],doc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a trained model we can classify a new unseen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For practical purposes we use a mocked up document but we can easily query Solr or another store to get the content we want to classify\n",
    "# Eventually all this should be served in as a web service \n",
    "\n",
    "#taken from https://rd-alliance.org/groups/farm-data-sharing-ofds-wg\n",
    "unseen_document = \"\"\"\n",
    "Farmers have the capability as they have never had before to critically evaluate management practices \n",
    "using field-scale replicated strip trials. Farmers have gained this powerful capability because yield \n",
    "monitors on combines enable accurate measurement of yields. Networks of farmers have become\n",
    "increasingly common to exploit the potential of yield monitors to evaluate management practices \n",
    "at the field level. Networks of farmers have also become increasingly common because farmers understand \n",
    "the power of evaluating management practices across many fields. Collection of results from strip trials \n",
    "across many fields requires protocols for data stewardship, that is, for data reporting, sharing and archiving. \n",
    "All farmer networks have developed data stewardship protocols. \"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(unseen_document))\n",
    "predicted_topics = lda_model[vec]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our model with PyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'DeprecationWarning')\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(corpus=corpus, topic_model=lda_model, dictionary=dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "-\n",
    "\n",
    "> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
