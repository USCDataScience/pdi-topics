{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGU Topic Modeling and Insights Data Visualizations\n",
    "\n",
    "This notebook does topic modeling on EGU abstracts from 2009 to 2018, the main premise is that the PDF files are organized in the same way, first the EGU copyright notice, then the abstract title, then the authors and lastly the abstract content.\n",
    "\n",
    "We parsed the PDFs using PDFMiner's utility pdf2txt\n",
    "\n",
    "```sh\n",
    "ls *.pdf | xargs -n1 -P8 bash -c 'pdf2txt.py -o output/$0.txt -t text $0'\n",
    "```\n",
    "\n",
    "The current notebook uses Solr to index the parsed content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Schema:**\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "       'year': year,\n",
    "       'file': fname,\n",
    "       'title' : title,\n",
    "       'entities': entities,\n",
    "       'abstract': abstract,\n",
    "       'category': category,\n",
    "       'sessions': full_session_code,\n",
    "       'presentation': presentation,\n",
    "       'timestamp': datetime.now().isoformat(),\n",
    "      }\n",
    "```\n",
    "\n",
    "* category: the main session id, CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* presentation: oral, poster, pico etc.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import scattertext as st\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query can be customized to use any solr endpoint\n",
    "\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "solr_query = 'q=year:2011%20OR%20year:2018&wt=json&rows=200&sort=random_1234%20desc'\n",
    "solr_url = solr_root + solr_query\n",
    "\n",
    "req = urllib.request.Request(solr_url)\n",
    "# parsing response\n",
    "r = urllib.request.urlopen(req).read()\n",
    "json_response = json.loads(r.decode('utf-8'))\n",
    "solr_documents = json_response['response']['docs']\n",
    "print(\"Processing {0} randomly sampled documents. \\n\".format(len(solr_documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words (used for Gensim only)\n",
    "my_stop_words = {\n",
    "                    'area', 'time', 'measurement', 'data', 'event', 'doc', 'group', 'research', \n",
    "                    'study', 'use', 'analisys', 'result', 'case', 'model', 'information',\n",
    "                    'metadata', 'content', 'sharing', 'previous', 'http', \n",
    "                    '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010',\n",
    "                    '2011', '2012', '2013', '2014','2015', '2016', '2017',\n",
    "                }\n",
    "\n",
    "stop_words = my_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document list will contain our corpus after cleaning it.\n",
    "scattertext_documents = []\n",
    "gensim_documents = []\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    title = clean_document(doc['title'][0])\n",
    "    if len(title) <= 10:\n",
    "        print(title)\n",
    "    if 'category' in doc:\n",
    "        category = doc['category'][0]\n",
    "    else:\n",
    "        category = 'NAN'\n",
    "    scattertext_documents.append({ 'text': ' '.join(document), \n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'session':category})\n",
    "    gensim_documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model using Gensim a library for topic modeling, the output is a list of topics present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENSIM \n",
    "num_passes = 5\n",
    "num_topics = 20\n",
    "words_per_topic = 6\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_documents)\n",
    "corpus = [dictionary.doc2bow(text) for text in gensim_documents]\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScatterText\n",
    "> Now we're going to use the ScatterText library to visualize some binary categories token distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our documents into a Panda dataframe for ScatterText\n",
    "df = pd.DataFrame.from_dict(scattertext_documents)\n",
    "axis_year = pd.DataFrame(df.groupby('year')['year'].count())\n",
    "axis_session = pd.DataFrame(df.groupby('session')['session'].count())\n",
    "print(axis_session, axis_year)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years to compare\n",
    "years = ['2018','2011']\n",
    "# scattertext categories (year or session)\n",
    "scatter_category = 'year'\n",
    "\n",
    "# We load the English vector space from https://spacy.io/models/\n",
    "nlp = spacy.load('en')\n",
    "# we plot the results to an HTML DS3 plot\n",
    "main_cat = '2018'\n",
    "compared_cat = '2011'\n",
    "    \n",
    "# We create a corpus using Scatter's built-in method.\n",
    "corpus = st.CorpusFromPandas(df, \n",
    "                             category_col=scatter_category, \n",
    "                             text_col='text',\n",
    "                             nlp=nlp).build()\n",
    "\n",
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category=main_cat,\n",
    "          category_name=main_cat,\n",
    "          not_category_name=compared_cat,\n",
    "          width_in_pixels=700)\n",
    "\n",
    "\n",
    "open(\"scattertext.html\", 'wb').write(html.encode('utf-8'))\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(IFrame(src='scattertext.html', width=900, height=800))\n",
    "print (\"plot ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can select a topic and then we'll print all the documents for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_of_interest = 11 # the topic index\n",
    "def getkey(doc):\n",
    "    return doc[1]\n",
    "\n",
    "print(\"Documents in Topic {0} ({1})\".format(topic_of_interest,topic_list[topic_of_interest]['terms']))\n",
    "for doc in sorted(topic_list[topic_of_interest]['documents'],key=getkey):\n",
    "    print(\" Document: {0} \\n - Probability:{1}\".format(doc[0],doc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a trained model we can classify a new unseen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For practical purposes we use a mocked up document but we can easily query Solr or another store to get the content we want to classify\n",
    "# Eventually all this should be served in as a web service \n",
    "\n",
    "#taken from https://rd-alliance.org/groups/farm-data-sharing-ofds-wg\n",
    "unseen_document = \"\"\"\n",
    "Farmers have the capability as they have never had before to critically evaluate management practices \n",
    "using field-scale replicated strip trials. Farmers have gained this powerful capability because yield \n",
    "monitors on combines enable accurate measurement of yields. Networks of farmers have become\n",
    "increasingly common to exploit the potential of yield monitors to evaluate management practices \n",
    "at the field level. Networks of farmers have also become increasingly common because farmers understand \n",
    "the power of evaluating management practices across many fields. Collection of results from strip trials \n",
    "across many fields requires protocols for data stewardship, that is, for data reporting, sharing and archiving. \n",
    "All farmer networks have developed data stewardship protocols. \"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(unseen_document))\n",
    "predicted_topics = lda_model[vec]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our model with PyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'DeprecationWarning')\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(corpus=corpus, topic_model=lda_model, dictionary=dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "-\n",
    "\n",
    "> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
