{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGU Topic Modeling and Insights Data Visualizations\n",
    "\n",
    "This notebook does topic modeling on EGU abstracts from 2009 to 2018, the main premise is that the PDF files are organized in the same way, first the EGU copyright notice, then the abstract title, then the authors and lastly the abstract content.\n",
    "\n",
    "We parsed the PDFs using PDFMiner's utility pdf2txt\n",
    "\n",
    "```sh\n",
    "ls *.pdf | xargs -n1 -P8 bash -c 'pdf2txt.py -o output/$0.txt -t text $0'\n",
    "```\n",
    "\n",
    "The current notebook uses Solr to index the parsed content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Schema:**\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "       'year': year,\n",
    "       'file': fname,\n",
    "       'title' : title,\n",
    "       'entities': entities,\n",
    "       'abstract': abstract,\n",
    "       'category': category,\n",
    "       'sessions': full_session_code,\n",
    "       'presentation': presentation,\n",
    "       'timestamp': datetime.now().isoformat(),\n",
    "      }\n",
    "```\n",
    "\n",
    "* category: the main session id, CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* presentation: oral, poster, pico etc.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/beto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import scattertext as st\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "import random\n",
    "import string\n",
    "pseudo_rand = [ random.choice(string.ascii_letters) for i in range(4)]\n",
    "seed = ''.join(pseudo_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying: \n",
      "http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&q=(year:*)%20AND%20(sessions:*CR%20OR%20sessions:*NH*)&wt=json&rows=2000&sort=random_*HbBu%20desc\n",
      "Processing 2000 randomly sampled documents out of 9541 total. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Querying Solr\n",
    "\n",
    "max_records = 2000\n",
    "years = ['*']\n",
    "sessions = ['CR','NH']\n",
    "\n",
    "# We sample Solr for up to MAX_RECORDS documents that comply with our criteria\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "solr_query = 'q=(year:{})%20AND%20(sessions:*{}*)&wt=json&rows={}&sort=random_*{}%20desc'.format(\\\n",
    "             '%20OR%20year:'.join(years),'%20OR%20sessions:*'.join(sessions), max_records, seed)\n",
    "solr_url = solr_root + solr_query\n",
    "print('Querying: \\n' + solr_url)\n",
    "\n",
    "req = urllib.request.Request(solr_url)\n",
    "# parsing response\n",
    "r = urllib.request.urlopen(req).read()\n",
    "json_response = json.loads(r.decode('utf-8'))\n",
    "solr_documents = json_response['response']['docs']\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} randomly sampled documents out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Cleaning our documents \n",
    "\n",
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words (used for Gensim only)\n",
    "my_stop_words = {\n",
    "                    'area', 'data', 'event', 'doc', 'group', 'research', \n",
    "                    'metadata', 'content', 'sharing', 'previous', 'http', \n",
    "                    '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010',\n",
    "                    '2011', '2012', '2013', '2014','2015', '2016', '2017',\n",
    "                }\n",
    "\n",
    "stop_words = my_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "scattertext_documents = []\n",
    "gensim_documents = []\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    if 'category' in doc:\n",
    "        category = doc['category'][0]\n",
    "    else:\n",
    "        category = 'NAN'\n",
    "    scattertext_documents.append({ 'id': doc['id'],\n",
    "                                   'text': ' '.join(document), \n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'session':category})\n",
    "    gensim_documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LDA model using Gensim a library for topic modeling, the output is a list of topics present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"rock\" + 0.007*\"earthquake\" + 0.005*\"study\" + 0.004*\"sediment\" + 0.004*\"result\" + 0.004*\"surface\"')\n",
      "(1, '0.009*\"ﬁre\" + 0.009*\"ﬂood\" + 0.008*\"damage\" + 0.008*\"hazard\" + 0.007*\"model\" + 0.006*\"tsunami\"')\n",
      "(2, '0.015*\"landslide\" + 0.012*\"risk\" + 0.009*\"hazard\" + 0.007*\"study\" + 0.006*\"model\" + 0.006*\"seismic\"')\n",
      "(3, '0.014*\"tsunami\" + 0.009*\"slope\" + 0.006*\"study\" + 0.005*\"water\" + 0.005*\"different\" + 0.005*\"earthquake\"')\n",
      "(4, '0.029*\"tsunami\" + 0.020*\"earthquake\" + 0.009*\"seismic\" + 0.008*\"source\" + 0.007*\"sea\" + 0.007*\"time\"')\n",
      "(5, '0.017*\"hazard\" + 0.010*\"risk\" + 0.008*\"volcanic\" + 0.007*\"eruption\" + 0.007*\"fault\" + 0.007*\"earthquake\"')\n",
      "(6, '0.011*\"tsunami\" + 0.006*\"study\" + 0.006*\"earthquake\" + 0.006*\"model\" + 0.005*\"risk\" + 0.005*\"result\"')\n",
      "(7, '0.009*\"risk\" + 0.007*\"water\" + 0.007*\"wave\" + 0.007*\"avalanche\" + 0.006*\"hazard\" + 0.006*\"natural\"')\n",
      "(8, '0.021*\"model\" + 0.017*\"landslide\" + 0.012*\"slope\" + 0.009*\"based\" + 0.008*\"study\" + 0.006*\"used\"')\n",
      "(9, '0.017*\"risk\" + 0.015*\"climate\" + 0.012*\"change\" + 0.009*\"ﬂood\" + 0.007*\"impact\" + 0.007*\"model\"')\n",
      "(10, '0.023*\"landslide\" + 0.016*\"model\" + 0.012*\"earthquake\" + 0.007*\"rainfall\" + 0.007*\"water\" + 0.006*\"time\"')\n",
      "(11, '0.009*\"precipitation\" + 0.007*\"earthquake\" + 0.006*\"analysis\" + 0.006*\"model\" + 0.006*\"result\" + 0.005*\"period\"')\n",
      "(12, '0.029*\"wave\" + 0.007*\"equation\" + 0.006*\"internal\" + 0.006*\"nonlinear\" + 0.005*\"result\" + 0.005*\"different\"')\n",
      "(13, '0.013*\"earthquake\" + 0.006*\"time\" + 0.005*\"map\" + 0.005*\"result\" + 0.005*\"study\" + 0.004*\"model\"')\n",
      "(14, '0.013*\"model\" + 0.013*\"river\" + 0.012*\"ﬂood\" + 0.009*\"level\" + 0.008*\"sea\" + 0.008*\"extreme\"')\n",
      "(15, '0.040*\"wave\" + 0.007*\"risk\" + 0.006*\"result\" + 0.005*\"different\" + 0.005*\"sea\" + 0.005*\"model\"')\n",
      "(16, '0.019*\"landslide\" + 0.007*\"high\" + 0.006*\"model\" + 0.006*\"slope\" + 0.005*\"image\" + 0.005*\"time\"')\n",
      "(17, '0.018*\"lightning\" + 0.006*\"activity\" + 0.006*\"thunderstorm\" + 0.005*\"cloud\" + 0.005*\"earthquake\" + 0.005*\"time\"')\n",
      "(18, '0.013*\"rock\" + 0.010*\"slope\" + 0.010*\"landslide\" + 0.008*\"model\" + 0.005*\"change\" + 0.005*\"soil\"')\n",
      "(19, '0.016*\"landslide\" + 0.008*\"hazard\" + 0.006*\"study\" + 0.006*\"seismic\" + 0.006*\"natural\" + 0.005*\"method\"')\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Using GENSIM to do topic modelling, this cell takes some time... hang on.\n",
    "\n",
    "num_passes = 5 #num pases should be adjusted, 5 is just a guesstimate of when convergence will be achieved.\n",
    "num_topics = 20\n",
    "words_per_topic = 6\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_documents)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in gensim_documents]\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScatterText\n",
    "* Now we're going to use the ScatterText library to visualize some binary categories token distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         session\n",
      "session         \n",
      "ML             1\n",
      "NH          1999       year\n",
      "year      \n",
      "2011   352\n",
      "2012   264\n",
      "2013   197\n",
      "2014   146\n",
      "2015   202\n",
      "2016   308\n",
      "2017   264\n",
      "2018   267\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: loading our documents into a Panda dataframe for ScatterText and listing the document distributions\n",
    "\n",
    "df = pd.DataFrame.from_dict(scattertext_documents)\n",
    "axis_year = pd.DataFrame(df.groupby('year')['year'].count())\n",
    "axis_session = pd.DataFrame(df.groupby('session')['session'].count())\n",
    "print(axis_session, axis_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Using ScatterText to compare 2 categories, this cell also takes time, if your browser ask you to wait... wait!\n",
    "\n",
    "# We are comparing:\n",
    "comparing = ['2012','2018']\n",
    "# scattertext categories (year or session)\n",
    "scatter_category = 'year'\n",
    "\n",
    "# We load the English vector space from https://spacy.io/models/\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "    \n",
    "# We create a corpus using Scatter's built-in method.\n",
    "scatter_corpus = st.CorpusFromPandas(df, \n",
    "                             category_col=scatter_category, \n",
    "                             text_col='text',\n",
    "                             nlp=nlp).build()\n",
    "\n",
    "html = st.produce_scattertext_explorer(scatter_corpus,\n",
    "          category=comparing[0],\n",
    "          category_name=comparing[0],\n",
    "          not_category_name=comparing[1],\n",
    "          metadata=scatter_corpus.get_df()['title'],\n",
    "          minimum_term_frequency=5,\n",
    "          width_in_pixels=700)\n",
    "\n",
    "\n",
    "open(\"scattertext.html\", 'wb').write(html.encode('utf-8'))\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "print (\"Loading plot...\")\n",
    "display(IFrame(src='scattertext.html', width=900, height=800))\n",
    "# The search box is not working, presumably because Jupyter getting in the way of scattertext js libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can select a topic and then we'll print all the documents for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a trained model we can classify a new unseen document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Classifying an unseen document using our GENSIM model\n",
    "\n",
    "# For practical purposes we use a mocked up document but we can easily query Solr or another store to get the content we want to classify\n",
    "# Eventually all this should be served in as a web service \n",
    "#taken from https://meetingorganizer.copernicus.org/EGU2018/EGU2014-2415.pdf\n",
    "\n",
    "unseen_document = \"\"\"\n",
    "Waves  in  the  Southern  Ocean  are  the  largest  in  the  planet.  In  the  Southern  Hemisphere,  the  absence  of  large\n",
    "landmasses at high latitudes allows the wind to feed energy into the ocean over a virtually unlimited fetch. The\n",
    "enormous amount air-sea momentum exchanged over the Southern Ocean plays a substantial role on the global\n",
    "climate. However, large biases affect the estimation of wave regime around the Antarctic continent making climate\n",
    "prediction susceptible to uncertainty.\n",
    " \"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(unseen_document))\n",
    "predicted_topics = lda_model[vec]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our model with PyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Using pyLDAvis to visualize our topic distributions in the principal component axis.\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus, topic_model=lda_model, dictionary=dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "> Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017. Link to preprint: arxiv.org/abs/1703.00565"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
