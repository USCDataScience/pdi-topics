{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Insights Data Visualizations for EGU sessions\n",
    "\n",
    "--- \n",
    "\n",
    "This notebook does topic modeling on EGU Sessions using the submitted abstracts each year, the objective is to see if a given sessions match the topics discovered on the abstracs themselves so the organizers can adjust the sessions to asign abstracts in a better way.\n",
    "\n",
    "One thing to notice is that topic modeling algorithms work better with a bigger more diverse corpus, the abstracts submitted to a particular category will have a lot of overlap and picking up topics is more of a challenge.\n",
    "\n",
    "The right questions may be, **if we were to reduce the number of individual sessions what that would look like?**\n",
    "Likewise, if we want to be more specific on the sessions' topics we could asjust the LDA algorithm to a higher cluster number and see the resulting topics. Now let's get to the code.\n",
    "\n",
    "The current notebook uses Solr to retrieve these asbtracts\n",
    "\n",
    "\n",
    "**Database Schema:**\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "\"entities\":[\n",
    "    \"Jeffrey Obelcz  and Warren T. Wood\",\n",
    "    \"NRC Postdoctoral Fellow\",\n",
    "    \"Naval Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States jbobelcz@gmail.com\",\n",
    "    \"Naval\",\n",
    "    \"Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States\"],\n",
    "\"id\": \"EGU2018-9778\",\n",
    "\"sessions\": [\"ESSI4.3\"],\n",
    "\"file\": [\"EGU2018-9778\"],\n",
    "\"presentation\": [\"Posters\"],\n",
    "\"year\": [2018],\n",
    "\"title\": [\"Towards a Quantitative Understanding of Parameters Driving Submarine Slope Failure: A Machine Learning Approach\"],\n",
    "\"category\": [\"ESSI\"],\n",
    "\"abstract\":[\"Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales ...\"]\n",
    "}\n",
    "```\n",
    "\n",
    "* category: the main session id, CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* presentation: oral, poster, pico etc.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Current Disciplinary Individual Sessions for Earth & Space Science Informatics (ESSI) 2018\n",
    "\n",
    "\n",
    "**ESSI1 – Community-driven challenges and solutions dealing with Informatics**\n",
    "\n",
    " * **ESSI1.1** - Informatics in Oceanography and Ocean Science \n",
    " * GI1.3/AS5.15/BG1.30/CL5.10/EMRP4.5/**ESSI1.6**/HS11.12/SM5.03 - Environmental sensor network (co-organized) \n",
    " * IE4.3/SSS13.73/AS5.19/BG1.20/**ESSI1.8**/HS11.4/NH11.13 - Geostatistical and statistical tools to perform the data fusion of large datasets in geo-engineering and environmental studies (co-organized)\n",
    " * NH9.12/AS5.17/CL5.30/**ESSI1.9**/GI0.4/GMPV6.12/HS11.44/SM3.15/SSS13.66 - Methods and Tools for Natural Risk Management and Communications – Innovative ways of delivering information to end users and sharing data among the scientific community (co-organized)\n",
    " * IE4.7/SSS13.74/BG1.43/**ESSI1.10**/NH9.21/SM1.10 -Media Citizen Science for Earth Systems in the Era of Big Data (co-organized)\n",
    "   \n",
    "**ESSI2 – Infrastructures across the Earth and Space Sciences**\n",
    "\n",
    " * **ESSI2.1** - Metadata, Data Models, Semantics, and Collaboration\n",
    " * **ESSI2.2** - Data cubes of Big Earth Data - a new paradigm for accessing and processing Earth Science Data\n",
    " * IE4.1/NP4.3/AS5.13/CL5.18/**ESSI2.3**/GD10.6/HS3.7/NH11.14/SM7.03 - Big data and machine learning in geosciences (co-organized)\n",
    " * **ESSI2.4** - Virtual Research Environments: creating online collaborative environments to support research in the Earth Sciences and beyond (co-organised with American Geophysical Union)\n",
    " * **ESSI2.6** - Web-based Exchange and Processing of Environmental Data\n",
    " * **ESSI2.7** - Future Shock: Evolving Earth Science Data and Information Systems across the entire research lifecycle\n",
    " * **ESSI2.8**/GI1.6 - Environmental physical and data infrastructures: practices, access and technologies - towards system level understanding (co-organized)\n",
    " * **ESSI2.9** - Integrating data and services in solid Earth sciences\n",
    " * GI1.1/EMRP4.3/**ESSI2.10**/SSS13.15 - Applications of Data, Methods and Models in Geosciences (co-organized)\n",
    " * GI1.5/EMRP4.6/**ESSI2.11**/NH11.10/PS5.5 - Data fusion, integration, correlation and advances of non-destructive testing methods and numerical developments for engineering and geosciences applications (co-organized)\n",
    " * IE4.5/AS5.14/BG1.22/CL5.26/EMRP4.35/**ESSI2.12**/GD10.7/GI1.7 - Information extraction from satellite observations using data-driven methods (co-organized)\n",
    "\n",
    "**ESSI3 – Open Science 2.0 Informatics for Earth and Space Sciences**\n",
    "\n",
    " * **ESSI3.1** - Free and Open Source Software (FOSS) for Geoinformatics and Geosciences\n",
    " * **ESSI3.2** - Innovative Evaluation and Prediction for Large Earth Science Datasets\n",
    " * **ESSI3,4** - Earth science on Cloud, HPC and Grid\n",
    " * **ESSI3.5** - Open Data, Reproducible Research, and Open Science\n",
    " \n",
    "**ESSI4 – Visualization for scientific discovery and communication**\n",
    "\n",
    " * **ESSI4.1**/SSS11.6 - State of the Art in Earth Science Data Visualization (co-organized)\n",
    " * SC2.6/**ESSI4.2** - Visualization in Earth Science: best practices (co-organized)\n",
    " * **ESSI4.3** - Advancing Data-driven Workflows, Analytics and Visualization in Earth System Science\n",
    " * IE3.1/GI0.3/BG1.35/CR2.8/**ESSI4.4**/GM2.12/NH6.5 - Close and Long Range Sensing of Environment (co-sponsored by ISPRS) (co-organized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import scattertext as st\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "import random\n",
    "\n",
    "import string\n",
    "pseudo_rand = [ random.choice(string.ascii_letters) for i in range(4)]\n",
    "seed = ''.join(pseudo_rand)\n",
    "\n",
    "\n",
    "# wordcloud dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Loading pretrained word2vec model from GloVe \n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('models/glove.6B.100d.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Querying Solr\n",
    "\n",
    "# terms = ['ice', 'climate'] to include only abstracts with specified terms\n",
    "terms = ['*']\n",
    "years = ['*']\n",
    "entities = ['*']\n",
    "sessions = ['NH']\n",
    "\n",
    "# We sample Solr for up to \"page_size\" documents that comply with our criteria\n",
    "page_size = 1000\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "\n",
    "if terms[0] != '*':\n",
    "    terms_wirldcard = ['*' + t + '*' for t in terms]\n",
    "else:\n",
    "    terms_wirldcard = ['*']\n",
    "    \n",
    "if sessions[0] != '*':\n",
    "    sessions_wirldcard = ['*' + s + '*' for s in sessions]\n",
    "else:\n",
    "    sessions_wirldcard = ['*']\n",
    "\n",
    "terms_query = '%20OR%20abstract:'.join(terms_wirldcard)\n",
    "years_query = '%20OR%20year:'.join(years)  \n",
    "entities_query = '%20OR%20entities:'.join(entities)\n",
    "sessions_query = '%20OR%20sessions:'.join(sessions_wirldcard)\n",
    "query_string = 'q=(abstract:{}%20AND%20abstract:/.{{2}}.*/%20AND%20NOT%20title:/.{{300}}.*/)%20AND%20(year:{})' + \\\n",
    "                '%20AND%20(entities:{})%20AND%20(sessions:{})&wt=json&rows={}&cursorMark={}&sort=id+asc'\n",
    "while (more_results):    \n",
    "    solr_query = query_string.format(terms_query,\n",
    "                                     years_query,\n",
    "                                     entities_query,\n",
    "                                     sessions_query,\n",
    "                                     page_size,\n",
    "                                     cursorMark)\n",
    "    solr_url = solr_root + solr_query\n",
    "    print('Querying: \\n' + solr_url)\n",
    "    req = urllib.request.Request(solr_url)\n",
    "    # parsing response\n",
    "    r = urllib.request.urlopen(req).read()\n",
    "    json_response = json.loads(r.decode('utf-8'))\n",
    "    solr_documents.extend(json_response['response']['docs'])\n",
    "    nextCursorMark = json_response['nextCursorMark']\n",
    "    if (nextCursorMark == cursorMark):\n",
    "        more_results = False\n",
    "        break\n",
    "    else: \n",
    "        cursorMark = nextCursorMark\n",
    "\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} documents out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Cleaning our documents \n",
    "\n",
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words (used for Gensim only)\n",
    "my_stop_words = {\n",
    "                    'area', 'data', 'event', 'doc', 'group', 'research', 'http', 'community', 'result', \n",
    "                    'metadata', 'content', 'sharing', 'previous', 'model', 'science', 'scientiﬁc', 'user'\n",
    "                }\n",
    "years = [str(year) for year in range(2000,2020)]\n",
    "words_and_years = my_stop_words.union(years)\n",
    "stop_words = words_and_years.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "document_list = []\n",
    "gensim_documents = []\n",
    "word_cloud_text_all = ''\n",
    "\n",
    "# artifact of parsing the sessions from the pdf documents\n",
    "garbage_str = '</a>'\n",
    "\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    if 'sessions' in doc:\n",
    "        sindex = doc['sessions'][0].find(garbage_str)\n",
    "        if sindex != -1:\n",
    "            sessions = doc['sessions'][0][0:sindex]\n",
    "        else: \n",
    "            sessions = doc['sessions'][0]\n",
    "    else:\n",
    "        sessions = 'NAN'\n",
    "    document_list.append({ 'id': doc['id'],\n",
    "                                   'text': ' '.join(document), \n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'category': doc['category'][0].replace('/', '').replace('<', ''),\n",
    "                                   'sessions':sessions})\n",
    "    gensim_documents.append(document)\n",
    "    word_cloud_text_all = word_cloud_text_all + ' '.join(document)\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_documents)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in gensim_documents]\n",
    "\n",
    "df = pd.DataFrame.from_dict(document_list)\n",
    "axis_category = pd.DataFrame(df.groupby(['category', 'year'])['category'].count()).rename(columns={'category': 'count'})\n",
    "print(axis_category.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Building the LDA model using Gensim a library for topic modeling, first we are going to reduce the sessions from 4 to 3 and see what are the topics listed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 5: LDA Topic Modeling\n",
    "\n",
    "# num pases should be adjusted, 5 is just a guesstimate of when convergence will be achieved.\n",
    "num_passes = 5 \n",
    "num_topics = 3\n",
    "words_per_topic = 7\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "print (\"Topic List: \\n\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "print (\"\\nPyLDAVis: \\n\")\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus, topic_model=lda_model, dictionary=dictionary, sort_topics=False, mds='tsne')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly if we were to reduce the main sessions to 3 we end up with less fancy names that the current ones but kind of make sense. This modeling is context-independent, we could bring some context via Word2Vec and will discuss that later on. Now let's make an experiment on the current corpus, we trained a simple model with 3 topics, let's classify some abstracts and see where they fall into.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Classifying an abstract using our GENSIM model\n",
    "\n",
    "# ESSI abstracts taken from https://meetingorganizer.copernicus.org/EGU2018/EGU2018-9778.pdf\n",
    "\n",
    "document = \"\"\"\n",
    "Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from \n",
    "continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales, \n",
    "from small (10e4-10e5 m3/event), sub-annual failures on heavily sedimented river deltas to margin-altering and \n",
    "tsunamigenic (10-100 km3/event) open slope failures occurring on glacial-interglacial timescales. \n",
    "Despite their importance to basic (closing the global source-to-sink sediment budget) and applied \n",
    "(submarine geohazards) re- search, submarine slope failure frequency and magnitude on most continental margins \n",
    "remains poorly constrained. This is primarily due to difﬁculty in 1) directly observing events, and 2) reconstructing \n",
    "age and size, particularly in the geologic record. The state of knowledge regarding submarine slope failure \n",
    "preconditioning and triggering factors is more qualitative than quantitative; a vague hierarchy of factor importance \n",
    "has been established in most settings but slope failures cannot yet be forecasted or hindcasted from \n",
    "a priori knowledge of these factors.\n",
    "\"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(document))\n",
    "predicted_topics = lda_model[vec]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's increment the number of topics to 8 and see what we get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 7: LDA Topic Modeling expanding our topics\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "p = re.compile('.(\\\".*\\\")')\n",
    "topic_list = defaultdict(list)\n",
    "# num pases should be adjusted, 5 is just a guesstimate of when convergence will be achieved.\n",
    "num_passes = 5\n",
    "num_topics = 20\n",
    "words_per_topic = 7\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus,\n",
    "                                     num_topics=num_topics,\n",
    "                                     id2word = dictionary,\n",
    "                                     passes=num_passes,\n",
    "                                     chunksize=17)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "print (\"Topic List:\\n\")\n",
    "for topic in topics:\n",
    "    weighted_terms = topic[1].split(' + ')\n",
    "    terms = [t[6:] for t in weighted_terms]\n",
    "    for term in terms:\n",
    "        topic_list[topics.index(topic)].append(term.replace('\"',''))\n",
    "    print(topic)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nPyLDAVis: \\n\")\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus, topic_model=lda_model, dictionary=dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8, let's infer some context using word2Vec\n",
    "\n",
    "for topic_number in range(20):\n",
    "    try:\n",
    "        sm = model.most_similar(topic_list[topic_number][0:3], topn=30)\n",
    "    except:\n",
    "        sm = model.most_similar(reversed(topic_list[topic_number][0:1]), topn=30)\n",
    "    similar_words = [w[0] for w in sm]\n",
    "    similar_words.extend(topic_list[topic_number])\n",
    "    word_cloud_text = ' '.join(similar_words)\n",
    "    print(word_cloud_text)\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        scale=4,\n",
    "        prefer_horizontal=0.60,\n",
    "        min_font_size=20,\n",
    "        max_font_size=80,\n",
    "        max_words=100,\n",
    "        background_color=\"white\").generate(word_cloud_text)\n",
    "    \n",
    "    wordcloud.to_file('topic-' + str(topic_number) + '.png')\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: let's create a wordcloud of the whole corpus\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(word_cloud_text_all)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 10: we list the abstracts and the predicted topics\n",
    "\n",
    "min_likelihood  = 0.1\n",
    "\n",
    "def createLink(doc):\n",
    "    baseURL = 'https://meetingorganizer.copernicus.org/EGU' + str(doc['year']) + '/' + doc['id'] + '.pdf'\n",
    "    return baseURL\n",
    "\n",
    "def classify(doc):\n",
    "    vec = dictionary.doc2bow(clean_document(doc))\n",
    "    predicted_topics = lda_model[vec]\n",
    "    return [p for p in predicted_topics if p[1]> min_likelihood]\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "for doc in document_list:\n",
    "    doc['predicted'] = classify(doc['text'])\n",
    "    display(HTML('<br>Abstract <a href=\"{}\" target=\"_blank\">{}</a> belongs to session {}, predicted in topics -> {}'.format(\n",
    "        createLink(doc),\n",
    "        doc['id'],\n",
    "        doc['sessions'],\n",
    "        doc['predicted'] )))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "> Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017. Link to preprint: arxiv.org/abs/1703.00565\n",
    "\n",
    "> Sievert, C & Shirley, K.E.. (2014). LDAvis: A method for visualizing and interpreting topics. Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces. 63-70. \n",
    "\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
