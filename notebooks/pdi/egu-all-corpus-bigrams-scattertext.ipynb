{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGU Topic Modeling and prediction\n",
    "\n",
    "This notebook uses topic modeling to analyze the EGU conference using the abstracts submitted for the years 2011 to 2018. The notebook can be used to detect trends and visualize topics along these years using one or many categories. The abstracts were parsed into text and ingested into a Apache Solr instance.\n",
    "\n",
    "We parsed the PDFs using PDFMiner's utility pdf2txt\n",
    "\n",
    "```sh\n",
    "ls *.pdf | xargs -n1 -P8 bash -c 'pdf2txt.py -o output/$0.txt -t text $0'\n",
    "```\n",
    "\n",
    "**Solr Schema**\n",
    " \n",
    " \n",
    " \n",
    "```json\n",
    "doc = {\n",
    "\"entities\":[\n",
    "    \"Jeffrey Obelcz  and Warren T. Wood\",\n",
    "    \"NRC Postdoctoral Fellow\",\n",
    "    \"Naval Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States jbobelcz@gmail.com\",\n",
    "    \"Naval\",\n",
    "    \"Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States\"],\n",
    "\"id\": \"EGU2018-9778\",\n",
    "\"sessions\": [\"ESSI4.3\"],\n",
    "\"file\": [\"EGU2018-9778\"],\n",
    "\"presentation\": [\"Posters\"],\n",
    "\"year\": [2018],\n",
    "\"title\": [\"Towards a Quantitative Understanding of Parameters Driving Submarine Slope Failure: A Machine Learning Approach\"],\n",
    "\"category\": [\"ESSI\"],\n",
    "\"abstract\":[\"Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales ...\"]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "## Application\n",
    "\n",
    "This notebook can be used to analyze what a corpora of scientific text talks about, in this case we used EGU abstracts but it can be used on any text corpora.\n",
    "\n",
    "The topics are examined using LDAVis, which displays the topics on an X-Y plot (intertopic distance map). Topic are represented by circles whose areas are proportional to the relative prevalences of the topics in the corpus. In the display the user can enter a topic number (note 1-base numbering vs. 0-base numbering in Topic List below cells 5 and 7); the terms are displayed on the right, ranked by significance (weight). A topic can be selected on the fly by hovering over its circle; clicking selects that topic. A user can also click on a term in the RH panel to show the topics in which that term occurs. \n",
    "\n",
    "The slider at the top of the RH panel allows the user to vary the “saliency”, i.e. uniqueness of the terms to that topic. A value of 0.6 is optimal, according to the authors of the algorithm. Blue bars represent overall term frequency while red bars show term frequency within the topic, which will be different when the saliency Is selected be less than one.\n",
    "\n",
    "Please see the annotated image of the LDAVis display.\n",
    "\n",
    "![](https://raw.githubusercontent.com/USCDataScience/pdi-topics/master/notebooks/pdi/img/pylda.png)\n",
    "\n",
    "\n",
    "\n",
    "The final cell of this notebook lists the titles and IDs of abstracts belonging to a specified topic.\n",
    "\n",
    "* category: the Program Group, e.g. CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* presentation: Presentation type: oral, poster, pico etc.\n",
    "* session: the session that the abstract appeared in; sessions may be co-organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "EGU Programme Groups:\n",
    "\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)\n",
    "\n",
    "#### Interdisciplinary Events (IE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "import urllib\n",
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "pseudo_rand = [random.choice(string.ascii_letters) for i in range(4)]\n",
    "seed = ''.join(pseudo_rand)\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# We are using spacy as a parser so we disable their other capabilities to speed up things\n",
    "nlp = spacy.load('en', disable=['tagger', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying: \n",
      "http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&q=(abstract:*%20AND%20abstract:/.{2}.*/%20AND%20NOT%20title:/.{300}.*/)%20AND%20(year:2011%20OR%20year:2018)%20AND%20(entities:*)%20AND%20(sessions:*NH*)&wt=json&rows=1000&cursorMark=*&sort=random_*zLAj%20desc,id%20desc\n",
      "Querying: \n",
      "http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&q=(abstract:*%20AND%20abstract:/.{2}.*/%20AND%20NOT%20title:/.{300}.*/)%20AND%20(year:2011%20OR%20year:2018)%20AND%20(entities:*)%20AND%20(sessions:*NH*)&wt=json&rows=1000&cursorMark=AoJQtYCoKSxFR1UyMDE4LTk3NjI=&sort=random_*zLAj%20desc,id%20desc\n",
      "Querying: \n",
      "http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&q=(abstract:*%20AND%20abstract:/.{2}.*/%20AND%20NOT%20title:/.{300}.*/)%20AND%20(year:2011%20OR%20year:2018)%20AND%20(entities:*)%20AND%20(sessions:*NH*)&wt=json&rows=1000&cursorMark=AoJT2oOCFCxFR1UyMDExLTE4MTY=&sort=random_*zLAj%20desc,id%20desc\n",
      "Querying: \n",
      "http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&q=(abstract:*%20AND%20abstract:/.{2}.*/%20AND%20NOT%20title:/.{300}.*/)%20AND%20(year:2011%20OR%20year:2018)%20AND%20(entities:*)%20AND%20(sessions:*NH*)&wt=json&rows=1000&cursorMark=AoJQ2ZwBLEVHVTIwMTgtMTg5OA==&sort=random_*zLAj%20desc,id%20desc\n",
      "Processing 2863 out of 2863 total. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Querying Solr\n",
    "\n",
    "# terms = ['ice', 'climate'] to include only abstracts with specified terms\n",
    "terms = ['*']\n",
    "years = ['2011', '2018']\n",
    "entities = ['*']\n",
    "sessions = ['NH']\n",
    "\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "\n",
    "if terms[0] != '*':\n",
    "    terms_wirldcard = ['*' + t + '*' for t in terms]\n",
    "else:\n",
    "    terms_wirldcard = ['*']\n",
    "    \n",
    "if sessions[0] != '*':\n",
    "    sessions_wirldcard = ['*' + s + '*' for s in sessions]\n",
    "else:\n",
    "    sessions_wirldcard = ['*']\n",
    "    \n",
    "if entities[0] != '*':\n",
    "    entities_wirldcard = ['*' + e + '*' for e in entities]\n",
    "else:\n",
    "    entities_wirldcard = ['*']\n",
    "\n",
    "# Return \"page_size\" documents with each Solr query until complete\n",
    "page_size = 1000\n",
    "terms_query = '%20OR%20abstract:'.join(terms_wirldcard)\n",
    "years_query = '%20OR%20year:'.join(years)  \n",
    "entities_query = '%20OR%20entities:'.join(entities_wirldcard)\n",
    "sessions_query = '%20OR%20sessions:'.join(sessions_wirldcard)\n",
    "query_string = 'q=(abstract:{}%20AND%20abstract:/.{{2}}.*/%20AND%20NOT%20title:/.{{300}}.*/)%20AND%20(year:{})' + \\\n",
    "                '%20AND%20(entities:{})%20AND%20(sessions:{})&wt=json&rows={}&cursorMark={}&sort=random_*{}%20desc,id%20desc'\n",
    "while (more_results):    \n",
    "    solr_query = query_string.format(terms_query,\n",
    "                                     years_query,\n",
    "                                     entities_query,\n",
    "                                     sessions_query,\n",
    "                                     page_size,\n",
    "                                     cursorMark,\n",
    "                                     seed)\n",
    "    solr_url = solr_root + solr_query\n",
    "    print('Querying: \\n' + solr_url)\n",
    "    req = urllib.request.Request(solr_url)\n",
    "    # parsing response\n",
    "    r = urllib.request.urlopen(req).read()\n",
    "    json_response = json.loads(r.decode('utf-8'))\n",
    "    solr_documents.extend(json_response['response']['docs'])\n",
    "    nextCursorMark = json_response['nextCursorMark']\n",
    "    if (nextCursorMark == cursorMark):\n",
    "        more_results = False\n",
    "        break\n",
    "    else: \n",
    "        cursorMark = nextCursorMark\n",
    "\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 52s, sys: 10min 8s, total: 19min\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cell 3: Cleaning our documents \n",
    "\n",
    "# Helper function\n",
    "def flatten(top_list):\n",
    "    for inner in top_list:\n",
    "        if isinstance(inner, (list,tuple)):\n",
    "            for j in flatten(inner):\n",
    "                yield j\n",
    "        else:\n",
    "            yield inner\n",
    "\n",
    "\n",
    "stop_years = {str(year) for year in range(2000,2020)}\n",
    "\n",
    "nlp.Defaults.stop_words |= stop_years\n",
    "nlp.Defaults.stop_words |= ENGLISH_STOP_WORDS\n",
    "\n",
    "nlp.Defaults.stop_words |= {\n",
    "                            'area', 'data', 'event', 'use', 'group', 'research', 'model',\n",
    "                            'metadata', 'content', 'sharing', 'previous', 'http','study',\n",
    "                            'datum', 'result', 'different', 'change'\n",
    "                           }\n",
    "\n",
    "# After we generate the bigrams there are terms that we might want to remove including bigrams.\n",
    "post_process_stopwords = {'sea_ice', 'water'}\n",
    "\n",
    "ALL_STOP_WORDS = ENGLISH_STOP_WORDS.union(nlp.Defaults.stop_words)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_sentences(doc):\n",
    "    doc = nlp(doc)\n",
    "    processed_sentences = []\n",
    "    for num, sentence in enumerate(doc.sents):\n",
    "        tokens = [token.lemma_.encode('ascii',errors='ignore').decode().strip().lower() for token in sentence if token.lemma_ not in string.punctuation]\n",
    "        cleaned_sentence = [token for token in tokens if token not in ALL_STOP_WORDS]\n",
    "        cleaned_sentence = [token for token in cleaned_sentence if token != '-PRON-']\n",
    "        processed_sentences.append(cleaned_sentence)\n",
    "    return processed_sentences\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "gensim_unigram_documents = []\n",
    "document_list = []\n",
    "# artifact of parsing the sessions from the pdf documents\n",
    "garbage_str = '</a>'\n",
    "\n",
    "for doc in solr_documents:\n",
    "    abstract = clean_sentences(doc['abstract'][0])\n",
    "    if 'sessions' in doc:\n",
    "        sindex = doc['sessions'][0].find(garbage_str)\n",
    "        if sindex != -1:\n",
    "            sessions = doc['sessions'][0][0:sindex]\n",
    "        else: \n",
    "            sessions = doc['sessions'][0]\n",
    "    else:\n",
    "        sessions = 'NAN'\n",
    "    document_list.append({ 'id': doc['id'],\n",
    "                                   'text': abstract,\n",
    "                                   'bigrams': '',\n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'category': doc['category'][0].replace('/', '').replace('<', ''),\n",
    "                                   'sessions':sessions})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               count\n",
      "category year       \n",
      "AS       2018     19\n",
      "CL       2018     29\n",
      "EMRP     2011     11\n",
      "         2018     18\n",
      "EOS      2011     40\n",
      "G        2011     37\n",
      "         2018     13\n",
      "GI       2018    159\n",
      "GM       2011     41\n",
      "         2018    122\n",
      "GMPV     2011     61\n",
      "         2018    119\n",
      "HS       2011    141\n",
      "         2018    113\n",
      "IE       2018     98\n",
      "NH       2011    884\n",
      "         2018    676\n",
      "NP       2018     19\n",
      "OS       2018      4\n",
      "PS       2018      8\n",
      "SC       2011      1\n",
      "SM       2011     39\n",
      "         2018     77\n",
      "SSP      2018     11\n",
      "SSS      2018     53\n",
      "ST       2011     21\n",
      "TS       2018     49\n"
     ]
    }
   ],
   "source": [
    "# Cell #4 creating the bigram corpus\n",
    "\n",
    "# unigram sentences is going to be used to train our bigram phraser\n",
    "unigram_sentences = []\n",
    "# bigram corpus will contain an array of documents and their tokens, with bigram tokens included\n",
    "bigram_corpus = []\n",
    "\n",
    "\n",
    "for doc in document_list:\n",
    "    for sentence in doc['text']:\n",
    "        unigram_sentences.append(sentence)\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences, min_count=2)\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "\n",
    "for doc in document_list:\n",
    "    bigram_sentences = []\n",
    "    for unigram_sentence in doc['text']:\n",
    "        bigram_sentence = ' '.join(bigram_phraser[unigram_sentence])\n",
    "        bigram_sentences.append(bigram_sentence)\n",
    "    doc['bigrams'] = ' '.join(flatten(bigram_sentences))\n",
    "\n",
    "\n",
    "for odc in document_list:\n",
    "    bigram_tokens = []\n",
    "    bigram_doc = []\n",
    "    for sentence in doc['bigrams']:\n",
    "        bigrams = sentence.split()\n",
    "        bigram_tokens.append([b for b in bigrams if b not in post_process_stopwords])\n",
    "    bigram_corpus.append(list(flatten(bigram_tokens)))\n",
    "\n",
    "df = pd.DataFrame.from_dict(document_list)\n",
    "axis_category = pd.DataFrame(df.groupby(['category', 'year'])['category'].count()).rename(columns={'category': 'count'})\n",
    "print(axis_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScatterText using bigrams as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading plot...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"scattertext.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4461453e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5, using ScatterText\n",
    "\n",
    "\n",
    "import scattertext as st\n",
    "top_documents = 2000\n",
    "\n",
    "# scattertext categories (year or session)\n",
    "scatter_category = 'year'\n",
    "\n",
    "if scatter_category == 'year':\n",
    "    comparing = [years[0],years[1]]\n",
    "else:\n",
    "    comparing = [sessions[0],sessions[1]]\n",
    "    scatter_category = 'category'\n",
    "\n",
    "    \n",
    "# We create a corpus using Scatter's built-in method.\n",
    "scatter_corpus = st.CorpusFromPandas(df[:top_documents], \n",
    "                             category_col=scatter_category, \n",
    "                             text_col='bigrams',\n",
    "                             nlp=nlp).build()\n",
    "\n",
    "html = st.produce_scattertext_explorer(scatter_corpus,\n",
    "          category=comparing[0],\n",
    "          category_name=comparing[0],\n",
    "          not_category_name=comparing[1],\n",
    "          metadata=scatter_corpus.get_df()['title'],\n",
    "          minimum_term_frequency=5,\n",
    "          width_in_pixels=700)\n",
    "\n",
    "open(\"scattertext.html\", 'wb').write(html.encode('utf-8'))\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "print (\"Loading plot...\")\n",
    "display(IFrame(src='scattertext.html', width=900, height=800))\n",
    "# The search box is not working, presumably because Jupyter getting in the way of scattertext js libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Listing papers containing a particular ngram\n",
    "\n",
    "# use unigrams, sea_ice becomes sea, ice etc\n",
    "terms = set(['social','resilience'])\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "def createLink(doc):\n",
    "    baseURL = 'https://meetingorganizer.copernicus.org/EGU' + str(doc['year']) + '/' + doc['id'] + '.pdf'\n",
    "    return baseURL\n",
    "\n",
    "# Ode to Python's comprehension lists\n",
    "matches  = [doc for doc in document_list if terms == set(flatten(doc['text'])).intersection(terms)]\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# Let's predict the first 10 documents\n",
    "for doc in matches[0:top_n]:\n",
    "    display(HTML('<br>Abstract <a href=\"{}\" target=\"_blank\">{}</a> '.format(\n",
    "        createLink(doc),\n",
    "        doc['id']))) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
