{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Insights Data Visualizations for EGU sessions\n",
    "\n",
    "--- \n",
    "\n",
    "This notebook does topic modeling on EGU Sessions using the submitted abstracts each year, the objective is to see if a given sessions match the topics discovered on the abstracs themselves so the organizers can adjust the sessions to asign abstracts in a better way.\n",
    "\n",
    "One thing to notice is that topic modeling algorithms work better with a bigger more diverse corpus, the abstracts submitted to a particular category will have a lot of overlap and picking up topics is more of a challenge.\n",
    "\n",
    "The right questions may be, **if we were to reduce the number of individual sessions what that would look like?**\n",
    "Likewise, if we want to be more specific on the sessions' topics we could asjust the LDA algorithm to a higher cluster number and see the resulting topics. Now let's get to the code.\n",
    "\n",
    "\n",
    "\n",
    "> The current notebook uses natural Hazards (NH) from 2011 to 2018 and models what the main topics have been through these years, the query can be modified to only take into account especific years or use a particular term. \n",
    "\n",
    "\n",
    "\n",
    "**Database Schema:**\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "\"entities\":[\n",
    "    \"Jeffrey Obelcz  and Warren T. Wood\",\n",
    "    \"NRC Postdoctoral Fellow\",\n",
    "    \"Naval Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States jbobelcz@gmail.com\",\n",
    "    \"Naval\",\n",
    "    \"Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States\"],\n",
    "\"id\": \"EGU2018-9778\",\n",
    "\"sessions\": [\"ESSI4.3\"],\n",
    "\"file\": [\"EGU2018-9778\"],\n",
    "\"presentation\": [\"Posters\"],\n",
    "\"year\": [2018],\n",
    "\"title\": [\"Towards a Quantitative Understanding of Parameters Driving Submarine Slope Failure: A Machine Learning Approach\"],\n",
    "\"category\": [\"ESSI\"],\n",
    "\"abstract\":[\"Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales ...\"]\n",
    "}\n",
    "```\n",
    "\n",
    "* **category**: the main disciplinary session id, CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* **presentation**: oral, poster, pico etc.\n",
    "* **session**: the individual session for a given abstract, these can be co-organized.\n",
    "\n",
    "-----\n",
    "\n",
    "EGU Programme Groups:\n",
    "\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)\n",
    "\n",
    "#### Interdisciplinary Events (IE)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from datetime import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "import random\n",
    "\n",
    "import string\n",
    "pseudo_rand = [ random.choice(string.ascii_letters) for i in range(4)]\n",
    "seed = ''.join(pseudo_rand)\n",
    "\n",
    "\n",
    "# wordcloud dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Loading pretrained word2vec model from GloVe, this takes time.\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('models/glove.6B.100d.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Querying Solr\n",
    "\n",
    "# terms = ['ice', 'climate'] to include only abstracts with specified terms\n",
    "terms = ['*']\n",
    "years = ['*']\n",
    "entities = ['*']\n",
    "sessions = ['NH']\n",
    "\n",
    "# Return \"page_size\" documents with each Solr query until complete\n",
    "page_size = 1000\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "\n",
    "if terms[0] != '*':\n",
    "    terms_wirldcard = ['*' + t + '*' for t in terms]\n",
    "else:\n",
    "    terms_wirldcard = ['*']\n",
    "    \n",
    "if sessions[0] != '*':\n",
    "    sessions_wirldcard = ['*' + s + '*' for s in sessions]\n",
    "else:\n",
    "    sessions_wirldcard = ['*']\n",
    "    \n",
    "if entities[0] != '*':\n",
    "    entities_wirldcard = ['*' + e + '*' for e in entities]\n",
    "else:\n",
    "    entities_wirldcard = ['*']\n",
    "\n",
    "terms_query = '%20OR%20abstract:'.join(terms_wirldcard)\n",
    "years_query = '%20OR%20year:'.join(years)  \n",
    "entities_query = '%20OR%20entities:'.join(entities_wirldcard)\n",
    "sessions_query = '%20OR%20sessions:'.join(sessions_wirldcard)\n",
    "query_string = 'q=(abstract:{}%20AND%20abstract:/.{{2}}.*/%20AND%20NOT%20title:/.{{300}}.*/)%20AND%20(year:{})' + \\\n",
    "                '%20AND%20(entities:{})%20AND%20(sessions:{})&wt=json&rows={}&cursorMark={}&sort=id+asc'\n",
    "while (more_results):    \n",
    "    solr_query = query_string.format(terms_query,\n",
    "                                     years_query,\n",
    "                                     entities_query,\n",
    "                                     sessions_query,\n",
    "                                     page_size,\n",
    "                                     cursorMark)\n",
    "    solr_url = solr_root + solr_query\n",
    "    print('Querying: \\n' + solr_url)\n",
    "    req = urllib.request.Request(solr_url)\n",
    "    # parsing response\n",
    "    r = urllib.request.urlopen(req).read()\n",
    "    json_response = json.loads(r.decode('utf-8'))\n",
    "    solr_documents.extend(json_response['response']['docs'])\n",
    "    nextCursorMark = json_response['nextCursorMark']\n",
    "    if (nextCursorMark == cursorMark):\n",
    "        more_results = False\n",
    "        break\n",
    "    else: \n",
    "        cursorMark = nextCursorMark\n",
    "\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} documents out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Cleaning our documents \n",
    "\n",
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words (used for Gensim only)\n",
    "my_stop_words = {\n",
    "                    'area', 'data', 'event', 'doc', 'group', 'research', 'http', 'community', 'result', \n",
    "                    'metadata', 'content', 'sharing', 'previous', 'model', 'science', 'scientiﬁc', 'user'\n",
    "                }\n",
    "years = [str(year) for year in range(2000,2020)]\n",
    "words_and_years = my_stop_words.union(years)\n",
    "stop_words = words_and_years.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i.encode('ascii',errors='ignore').decode()  for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "document_list = []\n",
    "gensim_documents = []\n",
    "word_cloud_text_all = ''\n",
    "\n",
    "# artifact of parsing the sessions from the pdf documents\n",
    "garbage_str = '</a>'\n",
    "\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    if 'sessions' in doc:\n",
    "        sindex = doc['sessions'][0].find(garbage_str)\n",
    "        if sindex != -1:\n",
    "            sessions = doc['sessions'][0][0:sindex]\n",
    "        else: \n",
    "            sessions = doc['sessions'][0]\n",
    "    else:\n",
    "        sessions = 'NAN'\n",
    "    document_list.append({ 'id': doc['id'],\n",
    "                                   'text': ' '.join(document), \n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'category': doc['category'][0].replace('/', '').replace('<', ''),\n",
    "                                   'sessions':sessions})\n",
    "    gensim_documents.append(document)\n",
    "    word_cloud_text_all = word_cloud_text_all + ' '.join(document)\n",
    "\n",
    "dictionary = corpora.Dictionary(gensim_documents)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in gensim_documents]\n",
    "\n",
    "df = pd.DataFrame.from_dict(document_list)\n",
    "axis_category = pd.DataFrame(df.groupby(['category', 'year'])['category'].count()).rename(columns={'category': 'count'})\n",
    "\n",
    "# We are now listing the sessions, since we picked a main dosciplinary session we should see \n",
    "# more documents under that, however there are many co-organized sessions and those will also be lister here.\n",
    "print(axis_category.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Building the LDA model using Gensim a library for topic modeling, first we are going to reduce the categories to 3 and see what are the topics listed.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 5: LDA Topic Modeling\n",
    "\n",
    "# num pases should be adjusted, 3 is just a guesstimate of when convergence will be achieved.\n",
    "num_passes = 3 \n",
    "num_topics = 3\n",
    "words_per_topic = 7\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "print (\"Topic List: \\n\")\n",
    "for topic in topics:\n",
    "    t = str((int(topic[0])+ 1))\n",
    "    print('Topic ' + t + ': ', topic[1:])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "print (\"\\nPyLDAVis: \\n\")\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus,\n",
    "                        topic_model=lda_model,\n",
    "                        dictionary=dictionary,\n",
    "                        sort_topics=False,\n",
    "                        mds='tsne')\n",
    "\n",
    "# we recomend to adjust lambda to 0.6 as is recommended by the paper authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to reduce the main sessions to 3 we end up with less fancy names that the current ones but kind of make sense. This modeling is context-independent, we could bring some context via Word2Vec and will discuss that later on. Now let's make an experiment on the current corpus, we trained a simple model with 3 topics, let's classify some abstracts and see where they fall into.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Classifying an abstract using our GENSIM model\n",
    "\n",
    "# ESSI abstracts taken from https://meetingorganizer.copernicus.org/EGU2018/EGU2018-9778.pdf\n",
    "\n",
    "document = \"\"\"\n",
    "Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from \n",
    "continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales, \n",
    "from small (10e4-10e5 m3/event), sub-annual failures on heavily sedimented river deltas to margin-altering and \n",
    "tsunamigenic (10-100 km3/event) open slope failures occurring on glacial-interglacial timescales. \n",
    "Despite their importance to basic (closing the global source-to-sink sediment budget) and applied \n",
    "(submarine geohazards) re- search, submarine slope failure frequency and magnitude on most continental margins \n",
    "remains poorly constrained. This is primarily due to difﬁculty in 1) directly observing events, and 2) reconstructing \n",
    "age and size, particularly in the geologic record. The state of knowledge regarding submarine slope failure \n",
    "preconditioning and triggering factors is more qualitative than quantitative; a vague hierarchy of factor importance \n",
    "has been established in most settings but slope failures cannot yet be forecasted or hindcasted from \n",
    "a priori knowledge of these factors.\n",
    "\"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(document))\n",
    "predicted_topics = lda_model[vec]\n",
    "predicted_topics = [(p[0]+1, p[1]) for p in predicted_topics]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's increment the number of topics to 20 and see what we get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 7: LDA Topic Modeling expanding our topics\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "p = re.compile('.(\\\".*\\\")')\n",
    "topic_list = defaultdict(list)\n",
    "# num pases should be adjusted, 3 is just a guesstimate of when convergence will be achieved.\n",
    "num_passes = 3\n",
    "num_topics = 20\n",
    "words_per_topic = 7\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(lda_corpus,\n",
    "                                     num_topics=num_topics,\n",
    "                                     id2word = dictionary,\n",
    "                                     passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "print (\"Topic List:\\n\")\n",
    "for topic in topics:\n",
    "    weighted_terms = topic[1].split(' + ')\n",
    "    terms = [t[6:] for t in weighted_terms]\n",
    "    for term in terms:\n",
    "        topic_list[topics.index(topic)].append(term.replace('\"',''))\n",
    "    t = str((int(topic[0])+ 1))\n",
    "    print('Topic ' + t + ': ', topic[1:])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nPyLDAVis: \\n\")\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus,\n",
    "                        topic_model=lda_model,\n",
    "                        dictionary=dictionary,\n",
    "                        sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8, let's infer some context using word2Vec\n",
    "\n",
    "# We are taking the 3 most salient terms for each topic and getting the 30 most similar words\n",
    "# from the pre-trained GloVe vector model. We visualize the results using a word-cloud\n",
    "\n",
    "num_words = 30\n",
    "\n",
    "# the size of the words in the wordcloud are determined by the word frequency, if the freqcuency is the same\n",
    "# they will be given random sizes.\n",
    "\n",
    "for topic_number in range(num_topics):\n",
    "    print('Word cloud for topic {} '.format(topic_number+1))\n",
    "    print(topic_list[topic_number][0:3])\n",
    "    file_name = 'topic-' + str(int(topic_number)+1) + '-'.join(topic_list[topic_number][0:3]) + '.png'\n",
    "    salient_terms = topic_list[topic_number][0:3]\n",
    "    sm = model.most_similar(salient_terms, topn=num_words)\n",
    "    similar_words = [w[0] for w in sm]\n",
    "    similar_words.extend(topic_list[topic_number])\n",
    "    word_cloud_text = ' '.join(similar_words)\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        scale=4,\n",
    "        prefer_horizontal=0.60,\n",
    "        min_font_size=20,\n",
    "        max_font_size=80,\n",
    "        max_words=100,\n",
    "        background_color=\"white\").generate(word_cloud_text)\n",
    "    \n",
    "    wordcloud.to_file('wordclouds/' + file_name)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: let's create a wordcloud of the whole corpus\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(word_cloud_text_all)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 10: we list the first 10 abstracts and the predicted topics\n",
    "\n",
    "min_likelihood  = 0.1\n",
    "\n",
    "def createLink(doc):\n",
    "    baseURL = 'https://meetingorganizer.copernicus.org/EGU' + str(doc['year']) + '/' + doc['id'] + '.pdf'\n",
    "    return baseURL\n",
    "\n",
    "def classify(doc):\n",
    "    vec = dictionary.doc2bow(clean_document(doc))\n",
    "    predicted_topics = lda_model[vec]\n",
    "    return [p for p in predicted_topics if p[1]> min_likelihood]\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# Let's predict the first 10 documents\n",
    "for doc in document_list[0:10]:\n",
    "    doc['predicted'] = classify(doc['text'])\n",
    "    display(HTML('<br>Abstract <a href=\"{}\" target=\"_blank\">{}</a> belongs to session {}, predicted in topics -> {}'.format(\n",
    "        createLink(doc),\n",
    "        doc['id'],\n",
    "        doc['sessions'],\n",
    "        doc['predicted'] )))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "> Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017. Link to preprint: arxiv.org/abs/1703.00565\n",
    "\n",
    "> Sievert, C & Shirley, K.E.. (2014). LDAvis: A method for visualizing and interpreting topics. Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces. 63-70. \n",
    "\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
