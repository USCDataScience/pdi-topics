{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGU Insights Data Visualizations With ScatterText\n",
    "\n",
    "This notebook plots binary comparisons on ScatterText, a tool for finding distinguishing terms in small-to-medium-sized corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Database Schema:**\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "\"entities\":[\n",
    "    \"Jeffrey Obelcz  and Warren T. Wood\",\n",
    "    \"NRC Postdoctoral Fellow\",\n",
    "    \"Naval Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States jbobelcz@gmail.com\",\n",
    "    \"Naval\",\n",
    "    \"Research Lab\",\n",
    "    \"Seaﬂoor Sciences\",\n",
    "    \"United States\"],\n",
    "\"id\": \"EGU2018-9778\",\n",
    "\"sessions\": [\"ESSI4.3\"],\n",
    "\"file\": [\"EGU2018-9778\"],\n",
    "\"presentation\": [\"Posters\"],\n",
    "\"year\": [2018],\n",
    "\"title\": [\"Towards a Quantitative Understanding of Parameters Driving Submarine Slope Failure: A Machine Learning Approach\"],\n",
    "\"category\": [\"ESSI\"],\n",
    "\"abstract\":[\"Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales ...\"]\n",
    "}\n",
    "```\n",
    "\n",
    "* **category**: the main disciplinary session id, CL, AS etc. Keep in mind that these codes have changed through the years.\n",
    "* **presentation**: oral, poster, pico etc.\n",
    "* **session**: the individual session for a given abstract, these can be co-organized.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Disciplinary Sessions \n",
    "\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "Atmospheric Sciences (AS) -\n",
    "Biogeosciences (BG) -\n",
    "Climate: Past, Present, Future (CL) -\n",
    "Cryospheric Sciences (CR) -\n",
    "Earth Magnetism & Rock Physics (EMRP) -\n",
    "Energy, Resources and the Environment (ERE) -\n",
    "Earth & Space Science Informatics (ESSI) -\n",
    "Geodesy (G) -\n",
    "Geodynamics (GD) -\n",
    "Geosciences Instrumentation & Data Systems (GI) -\n",
    "Geomorphology (GM) -\n",
    "Geochemistry, Mineralogy, Petrology & Volcanology (GMPV) -\n",
    "Hydrological Sciences (HS) -\n",
    "Natural Hazards (NH) -\n",
    "Nonlinear Processes in Geosciences (NP) -\n",
    "Ocean Sciences (OS) -\n",
    "Planetary & Solar System Sciences (PS) -\n",
    "Seismology (SM) -\n",
    "Stratigraphy, Sedimentology & Palaeontology (SSP) -\n",
    "Soil System Sciences (SSS) -\n",
    "Solar-Terrestrial Sciences (ST) -\n",
    "Tectonics & Structural Geology (TS) -\n",
    "\n",
    "### Union Sessions\n",
    "\n",
    "Union Symposia (US)\n",
    "Great Debates (GDB)\n",
    "Medal Lectures (ML)\n",
    "Short courses (SC)\n",
    "Educational and Outreach Symposia (EOS)\n",
    "EGU Plenary, Ceremonies and Networking (PCN)\n",
    "Feedback and administrative meetings (FAM)\n",
    "Townhall and splinter meetings (TSM)\n",
    "Side events (SEV)\n",
    "Press conferences (PC)\n",
    "\n",
    "#### Interdisciplinary Events (IE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import urllib\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import scattertext as st\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "pseudo_rand = [ random.choice(string.ascii_letters) for i in range(4)]\n",
    "seed = ''.join(pseudo_rand)\n",
    "# We load the English vector space from https://spacy.io/models/\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Querying Solr\n",
    "\n",
    "# terms = ['ice', 'climate'] to include only abstracts with specified terms\n",
    "terms = ['*']\n",
    "entities = ['*']\n",
    "\n",
    "# the following parameters affect scatterText, use only up to 2 values i.e. year comparisons or category\n",
    "years = ['2012', '2018']\n",
    "sessions = ['NH']\n",
    "\n",
    "# scattertext categories (year or session)\n",
    "scatter_category = 'year'\n",
    "\n",
    "if scatter_category == 'year':\n",
    "    comparing = [years[0],years[1]]\n",
    "else:\n",
    "    comparing = [sessions[0],sessions[1]]\n",
    "\n",
    "# We sample Solr for up to \"page_size\" documents that comply with our criteria\n",
    "page_size = 400\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/egu/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "\n",
    "if terms[0] != '*':\n",
    "    terms_wirldcard = ['*' + t + '*' for t in terms]\n",
    "else:\n",
    "    terms_wirldcard = ['*']\n",
    "    \n",
    "if sessions[0] != '*':\n",
    "    sessions_wirldcard = ['*' + s + '*' for s in sessions]\n",
    "else:\n",
    "    sessions_wirldcard = ['*']\n",
    "    \n",
    "if entities[0] != '*':\n",
    "    entities_wirldcard = ['*' + e + '*' for e in entities]\n",
    "else:\n",
    "    entities_wirldcard = ['*']\n",
    "\n",
    "terms_query = '%20OR%20abstract:'.join(terms_wirldcard)\n",
    "years_query = '%20OR%20year:'.join(years)  \n",
    "entities_query = '%20OR%20entities:'.join(entities_wirldcard)\n",
    "sessions_query = '%20OR%20sessions:'.join(sessions_wirldcard)\n",
    "query_string = 'q=(abstract:{}%20AND%20abstract:/.{{2}}.*/%20AND%20NOT%20title:/.{{300}}.*/)%20AND%20(year:{})' + \\\n",
    "                '%20AND%20(entities:{})%20AND%20(sessions:{})&wt=json&rows={}&sort=random_*{}%20desc'\n",
    " \n",
    "solr_query = query_string.format(terms_query,\n",
    "                                 years_query,\n",
    "                                 entities_query,\n",
    "                                 sessions_query,\n",
    "                                 page_size,\n",
    "                                 seed)\n",
    "solr_url = solr_root + solr_query\n",
    "print('Querying: \\n' + solr_url)\n",
    "req = urllib.request.Request(solr_url)\n",
    "# parsing response\n",
    "r = urllib.request.urlopen(req).read()\n",
    "json_response = json.loads(r.decode('utf-8'))\n",
    "solr_documents.extend(json_response['response']['docs'])\n",
    "    \n",
    "\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} documents out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Cleaning our documents \n",
    "\n",
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words\n",
    "my_stop_words = {\n",
    "                    'area', 'data', 'event', 'doc', 'group', 'research', \n",
    "                    'metadata', 'content', 'sharing', 'previous', 'http'\n",
    "                }\n",
    "\n",
    "years = [str(year) for year in range(2000,2020)]\n",
    "words_and_years = my_stop_words.union(years)\n",
    "stop_words = words_and_years.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i.encode('ascii',errors='ignore').decode() for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "scattertext_documents = []\n",
    "\n",
    "garbage_str = '</a>'\n",
    "\n",
    "for doc in solr_documents:\n",
    "    document = clean_document(doc['abstract'][0])\n",
    "    if 'sessions' in doc:\n",
    "        sindex = doc['sessions'][0].find(garbage_str)\n",
    "        if sindex != -1:\n",
    "            sessions = doc['sessions'][0][0:sindex]\n",
    "        else: \n",
    "            sessions = doc['sessions'][0]\n",
    "    else:\n",
    "        sessions = 'NAN'\n",
    "    scattertext_documents.append({ 'id': doc['id'],\n",
    "                                   'text': ' '.join(document), \n",
    "                                   'year': str(doc['year'][0]),\n",
    "                                   'title': doc['title'][0],\n",
    "                                   'category': doc['category'][0].replace('/', '').replace('<', ''),\n",
    "                                   'sessions': sessions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScatterText\n",
    "\n",
    "* we're going to use the ScatterText library to visualize some binary categories token distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: loading our documents into a Panda dataframe for ScatterText and listing the document distributions\n",
    "\n",
    "df = pd.DataFrame.from_dict(scattertext_documents)\n",
    "axis_category = pd.DataFrame(df.groupby(['category', 'year'])['category'].count()).rename(columns={'category': 'count'})\n",
    "print(axis_category.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Using ScatterText to compare 2 categories, this cell also takes time, if your browser ask you to wait... wait!\n",
    "\n",
    "# We create a corpus using Scatter's built-in method.\n",
    "scatter_corpus = st.CorpusFromPandas(df, \n",
    "                             category_col=scatter_category, \n",
    "                             text_col='text',\n",
    "                             nlp=nlp).build()\n",
    "\n",
    "html = st.produce_scattertext_explorer(scatter_corpus,\n",
    "          category=comparing[0],\n",
    "          category_name=comparing[0],\n",
    "          not_category_name=comparing[1],\n",
    "          metadata=scatter_corpus.get_df()['title'],\n",
    "          minimum_term_frequency=5,\n",
    "          width_in_pixels=700)\n",
    "\n",
    "open(\"scattertext.html\", 'wb').write(html.encode('utf-8'))\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "print (\"Loading plot...\")\n",
    "display(IFrame(src='scattertext.html', width=900, height=800))\n",
    "# The search box is not working, presumably because Jupyter getting in the way of scattertext js libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "> Jason S. Kessler. Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. ACL System Demonstrations. 2017. Link to preprint: arxiv.org/abs/1703.00565"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
