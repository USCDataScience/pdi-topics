{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Insights Data Visualizations for ASSW abstracts.\n",
    "\n",
    "--- \n",
    "\n",
    "This notebook uses topic modeling to analyze ASSW's abstracts. The notebook can be used to see how well the assigned sessions match the topics discovered from the abstracts themselves, so the organizers can adjust the sessions to improve how abstracts are assigned.\n",
    "\n",
    "Note that topic modeling algorithms work best with a large, diverse corpus. The topics can be explored using LDAVis, which displays the topics in an X-Y plot (intertopic distance map). Topic are represented by circles whose areas are proportional to the relative prevalences of the topics in the corpus. In the display the user can enter a topic number (note 1-base numbering vs. 0-base numbering in Topic List below cells 5 and 7); the terms are displayed on the right, ranked by significance (weight). A topic can be selected on the fly by hovering over its circle; clicking selects that topic. A user can also click on a term in the RH panel to show the topics in which that term occurs. \n",
    "\n",
    "The slider at the top of the RH panel allows the user to vary the “saliency”, i.e. uniqueness of the terms to that topic. A value of 0.6 is optimal, according to the authors of the algorithm. Blue bars represent overall term frequency while red bars show term frequency within the topic, which will be different when the saliency Is selected be less than one.\n",
    "\n",
    "Please see the annotated image of the LDAVis display\n",
    "\n",
    "![](https://raw.githubusercontent.com/USCDataScience/pdi-topics/master/notebooks/pdi/img/pylda.png)\n",
    "\n",
    "\n",
    "To provide a quick visual representation of the topics the notebook uses word2vec to infer additional terms and then creates wordclouds out of these words.\n",
    "\n",
    "\n",
    "\n",
    "**Database Schema:**\n",
    "\n",
    "Example:\n",
    "\n",
    "```json\n",
    "doc = {\n",
    "        \"abstract\":[\"Clouds play a key role in the energy balance of the atmosphere due to their radiative effects, and have a critical  influence on the ice sheet's radiation budget. Changes in the glacier system on the Antarctic Peninsula (AP)  have been observed: disintegration of ice shelves, acceleration and thinning of glaciers, variations in the limits  between glacier faces and retreat of glacier fronts... \"],\n",
    "        \"entities\":[\"Marta Caballero1 (marta.caballero@fau.de)\",\n",
    "          \" Matthias Braun1\",\n",
    "          \" Thomas Mölg1 \",\n",
    "          \"1Friedrich Alexander-Universität Erlangen-Nürnberg\",\n",
    "          \" Institut für Geographie\",\n",
    "          \" Erlangen\",\n",
    "          \" Germany \"],\n",
    "        \"title\":[\"Evaluation of Satellite Derived Cloud Top Properties on the Antarctic Peninsula \"],\n",
    "        \"id\":\"Tue_30_AC-1_377 \",\n",
    "        \"_version_\":1621681835183439872}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import requirements\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import urllib\n",
    "import json\n",
    "import string\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from os import path\n",
    "\n",
    "\n",
    "# wordcloud dependencies\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# Heavy imports\n",
    "# from gensim.models import KeyedVectors\n",
    "# pretrained_model = KeyedVectors.load_word2vec_format('models/glove.6B.100d.vec', binary=False)\n",
    "nlp = spacy.load('en', disable=['tagger', 'ner'])\n",
    "# nlp = spacy.load('en')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Querying Solr\n",
    "\n",
    "# terms = ['ice', 'climate'] to include only abstracts with specified terms\n",
    "terms = ['*']\n",
    "entities = ['*']\n",
    "\n",
    "# Return \"page_size\" documents with each Solr query until complete\n",
    "page_size = 1000\n",
    "cursorMark = '*'\n",
    "\n",
    "solr_documents = []\n",
    "solr_root = 'http://integration.pdi-solr.labs.nsidc.org/solr/assw/select?indent=on&'\n",
    "more_results = True\n",
    "\n",
    "\n",
    "if terms[0] != '*':\n",
    "    terms_wirldcard = ['*' + t + '*' for t in terms]\n",
    "else:\n",
    "    terms_wirldcard = ['*']\n",
    "\n",
    "    \n",
    "if entities[0] != '*':\n",
    "    entities_wirldcard = ['*' + e + '*' for e in entities]\n",
    "else:\n",
    "    entities_wirldcard = ['*']\n",
    "\n",
    "terms_query = '%20OR%20abstract:'.join(terms_wirldcard)\n",
    "entities_query = '%20OR%20entities:'.join(entities_wirldcard)\n",
    "\n",
    "query_string = 'q=(abstract:{}%20AND%20abstract:/.{{2}}.*/%20AND%20NOT%20title:/.{{300}}.*/)' + \\\n",
    "                '%20AND%20(entities:{})&wt=json&rows={}&cursorMark={}&sort=id+asc'\n",
    "while (more_results):    \n",
    "    solr_query = query_string.format(terms_query,\n",
    "                                     entities_query,\n",
    "                                     page_size,\n",
    "                                     cursorMark)\n",
    "    solr_url = solr_root + solr_query\n",
    "    print('Querying: \\n' + solr_url)\n",
    "    req = urllib.request.Request(solr_url)\n",
    "    # parsing response\n",
    "    r = urllib.request.urlopen(req).read()\n",
    "    json_response = json.loads(r.decode('utf-8'))\n",
    "    solr_documents.extend(json_response['response']['docs'])\n",
    "    nextCursorMark = json_response['nextCursorMark']\n",
    "    if (nextCursorMark == cursorMark):\n",
    "        more_results = False\n",
    "        break\n",
    "    else: \n",
    "        cursorMark = nextCursorMark\n",
    "\n",
    "total_found = json_response['response']['numFound']\n",
    "print(\"Processing {0} documents out of {1} total. \\n\".format(len(solr_documents), total_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = {str(year) for year in range(2000,2020)}\n",
    "\n",
    "nlp.Defaults.stop_words |= years\n",
    "nlp.Defaults.stop_words |= ENGLISH_STOP_WORDS\n",
    "\n",
    "nlp.Defaults.stop_words |= {'data', 'area', 'interesting', 'water', 'region', 'using', 'different', 'science'\n",
    "                 'change', 'result', 'research', 'technique', 'datum', 'model', 'use',\n",
    "                 'observation', 'measurement', 'sample', 'study', 'analysis',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cell 4: Cleaning our documents  \n",
    "%load_ext line_profiler\n",
    "\n",
    "\n",
    "ALL_STOP_WORDS = ENGLISH_STOP_WORDS.union(nlp.Defaults.stop_words)\n",
    "\n",
    "def flatten(top_list):\n",
    "    for inner in top_list:\n",
    "        if isinstance(inner, (list,tuple)):\n",
    "            for j in flatten(inner):\n",
    "                yield j\n",
    "        else:\n",
    "            yield inner\n",
    "\n",
    "# Function to clean up the documents, lematizes the words to their regular form and removes the stop words.\n",
    "def clean_document(doc):\n",
    "    processed_sentences = []\n",
    "    raw_sentences = []\n",
    "    for num, sentence in enumerate(doc.sents):\n",
    "        raw_sentences.append([item for item in str(sentence).split(' ')])\n",
    "        tokens = [token.lemma_.encode('ascii',errors='ignore').decode().strip().lower() for token in sentence if token.lemma_ not in string.punctuation]\n",
    "        cleaned_sentence = [token for token in tokens if token not in ALL_STOP_WORDS]\n",
    "#         cleaned_sentence = [token for token in cleaned_sentence if len(token) > 1 ]\n",
    "        cleaned_sentence = [token for token in cleaned_sentence if token != '-PRON-']\n",
    "        processed_sentences.append(cleaned_sentence)\n",
    "    return (raw_sentences, processed_sentences)\n",
    "\n",
    "# bigram_corpus will contain our corpus after cleaning it.\n",
    "\n",
    "unigram_documents = []\n",
    "\n",
    "texts = [doc['abstract'][0] for doc in solr_documents]\n",
    "document_list = []\n",
    "\n",
    "for doc in nlp.pipe(texts, batch_size=200, n_threads=4):\n",
    "    assert doc.is_parsed\n",
    "    raw_sentences, cleaned_sentences = clean_document(doc)\n",
    "    unigram_documents.append(cleaned_sentences)\n",
    "    document_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the corpus in sentences for training\n",
    "unigram_sentences = []\n",
    "bigram_corpus = []\n",
    "bigram_docs = []\n",
    "\n",
    "for doc in unigram_documents:\n",
    "    for sentence in doc:\n",
    "        unigram_sentences.append(sentence)\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences, min_count=2)\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "\n",
    "for unigram_doc in unigram_documents:\n",
    "    bigram_sentences = []\n",
    "    for unigram_sentence in unigram_doc:\n",
    "        bigram_sentence = ' '.join(bigram_phraser[unigram_sentence])\n",
    "        bigram_sentences.append(bigram_sentence)\n",
    "    bigram_docs.append(bigram_sentences)\n",
    "\n",
    "\n",
    "for bigram_sentdoc in bigram_docs:\n",
    "    bigram_tokens = []\n",
    "    bigram_doc = []\n",
    "    for sentence in bigram_sentdoc:\n",
    "        bigram_tokens.append(sentence.split())\n",
    "    bigram_corpus.append(list(flatten(bigram_tokens)))\n",
    "\n",
    "print(bigram_corpus[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Building the LDA model using bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 5: LDA Topic Modeling\n",
    "\n",
    "# num pases should be adjusted, 3 is just a guesstimate of when convergence will be achieved.\n",
    "num_passes = 3\n",
    "num_topics = 20\n",
    "words_per_topic = 7\n",
    "\n",
    "dictionary = corpora.Dictionary(bigram_corpus)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in bigram_corpus]\n",
    "\n",
    "lda_model = LdaMulticore(lda_corpus,\n",
    "                         num_topics=num_topics,\n",
    "                         id2word=dictionary,\n",
    "                         passes=num_passes,\n",
    "                         workers=2\n",
    "                        )\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "print (\"Topic List: \\n\")\n",
    "for topic in topics:\n",
    "    t = str((int(topic[0])+ 1))\n",
    "    print('Topic ' + t + ': ', topic[1:])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "print (\"\\nPyLDAVis: \\n\")\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(corpus=lda_corpus,\n",
    "                        topic_model=lda_model,\n",
    "                        dictionary=dictionary,\n",
    "                        sort_topics=False)\n",
    "\n",
    "# we recomend to adjust lambda to 0.6 as is recommended by the paper authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Classifying an abstract using our GENSIM model\n",
    "\n",
    "# ESSI abstracts taken from https://meetingorganizer.copernicus.org/EGU2018/EGU2018-9778.pdf\n",
    "\n",
    "document = \"\"\"\n",
    "Submarine slope failure is a ubiquitous process and dominant pathway for sediment and organic carbon ﬂux from \n",
    "continental margins to the deep sea. Slope failure occurs over a wide range of temporal and spatial scales, \n",
    "from small (10e4-10e5 m3/event), sub-annual failures on heavily sedimented river deltas to margin-altering and \n",
    "tsunamigenic (10-100 km3/event) open slope failures occurring on glacial-interglacial timescales. \n",
    "Despite their importance to basic (closing the global source-to-sink sediment budget) and applied \n",
    "(submarine geohazards) re- search, submarine slope failure frequency and magnitude on most continental margins \n",
    "remains poorly constrained. This is primarily due to difﬁculty in 1) directly observing events, and 2) reconstructing \n",
    "age and size, particularly in the geologic record. The state of knowledge regarding submarine slope failure \n",
    "preconditioning and triggering factors is more qualitative than quantitative; a vague hierarchy of factor importance \n",
    "has been established in most settings but slope failures cannot yet be forecasted or hindcasted from \n",
    "a priori knowledge of these factors.\n",
    "\"\"\"\n",
    "\n",
    "vec = dictionary.doc2bow(clean_document(nlp(document)))\n",
    "predicted_topics = lda_model[vec]\n",
    "predicted_topics = [(p[0]+1, p[1]) for p in predicted_topics]\n",
    "print(predicted_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Links\n",
    "\n",
    "\n",
    "\n",
    "> L. A. Lopez, R. Duerr and S. J. S. Khalsa, \"Optimizing apache nutch for domain specific crawling at large scale,\" 2015 IEEE International Conference on Big Data (Big Data), Santa Clara, CA, 2015, pp. 1967-1971.\n",
    "doi: 10.1109/BigData.2015.7363976\n",
    "\n",
    "> Sievert, C & Shirley, K.E.. (2014). LDAvis: A method for visualizing and interpreting topics. Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces. 63-70. \n",
    "\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
