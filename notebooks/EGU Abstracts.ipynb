{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does topic modeling on EGU abstracts from the 2018 EGU meeting, the main premise is that the PDF files are organized in the same way, first the EGU copyright notice, then the abstract title, then the authors and lastly the abstract content.\n",
    "\n",
    "We parse the PDFs using PDFMiner's utility pdf2txt\n",
    "\n",
    "```sh\n",
    "ls *.pdf | xargs -n1 -P8 bash -c 'pdf2txt.py -o output/$0.txt -t text $0'\n",
    "```\n",
    "\n",
    "The current notebook uses abstracts from the atmospheric science interest group (AS) thus the modeling is not going to be illustrative of the EGU conference as a whole but rather a sub topic modelling exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/beto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We need to download nltk's wordnet first\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a list of objects containing the title, the entities and the abstract.\n",
    "# For now we only use the abstract content but later we can do NER training with the corpus using the entities field\n",
    "files_path = './abstracts/output/*.txt'\n",
    "files = glob.glob(files_path)\n",
    "documents = []\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(file) as f:\n",
    "            data = f.read()\n",
    "            # We split the paragraphs\n",
    "            item = data.split('\\n\\n')\n",
    "            title = item[1]\n",
    "            # Cleaning the entities, i.e. \"John Dow, (1) U of Colorado -> 'John dow','U of colorado'\"\n",
    "            entities = item[2].replace('(', '').replace(')', '').replace('\\n', ',').split(',')\n",
    "            entities = [e for e in entities if len(e)>2]\n",
    "            entities = [''.join([i for i in s if not i.isdigit()]) for s in entities]\n",
    "            abstract = item[3].replace('\\n', ' ')\n",
    "            doc = {\n",
    "                'file': f.name,\n",
    "                'title' : title,\n",
    "                'entities': entities,\n",
    "                'abstract': abstract\n",
    "            }\n",
    "            documents.append(doc)\n",
    "    except IOError as exc: #Not sure what error this is\n",
    "        if exc.errno != errno.EISDIR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words\n",
    "my_stop_words = {\n",
    "                    'http', 'www', 'area', 'time', 'measurement', 'data', 'event', 'service',\n",
    "                    'group', 'research', 'study', 'use', 'work', 'member', 'case',\n",
    "                    'meeting', 'news', 'model', 'project', 'standard',\n",
    "                    'statement', 'school', 'university', 'output', 'brokering',\n",
    "                    'repository', 'user', 'citation', 'chair', 'framework', 'information',\n",
    "                    'metadata', 'content', 'sharing', 'pid'\n",
    "                }\n",
    "stop_words = my_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# document list will contain our corpus after cleaning it.\n",
    "document_list = []\n",
    "# pairs is a list of the urls and the size of their content\n",
    "pairs = []\n",
    "# just the documents urls\n",
    "urls = []\n",
    "\n",
    "def clean_document(doc):\n",
    "    tokens = tokenizer.tokenize((doc).lower())\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    return document\n",
    "\n",
    "for doc in documents:\n",
    "    document = clean_document(doc['abstract'])\n",
    "    document_list.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.010*\"solar\" + 0.007*\"mission\" + 0.007*\"hail\" + 0.006*\"particle\" + 0.006*\"sep\" + 0.004*\"surface\"')\n",
      "(1, '0.005*\"surface\" + 0.005*\"huygens\" + 0.005*\"2017\" + 0.004*\"lightning\" + 0.004*\"reconnection\" + 0.004*\"region\"')\n",
      "(2, '0.012*\"climate\" + 0.008*\"change\" + 0.008*\"surface\" + 0.007*\"temperature\" + 0.006*\"result\" + 0.005*\"emission\"')\n",
      "(3, '0.007*\"climate\" + 0.006*\"ocean\" + 0.005*\"earth\" + 0.005*\"high\" + 0.005*\"impact\" + 0.005*\"global\"')\n",
      "(4, '0.009*\"emission\" + 0.006*\"wave\" + 0.006*\"high\" + 0.005*\"scale\" + 0.005*\"region\" + 0.004*\"atmospheric\"')\n",
      "(5, '0.019*\"ozone\" + 0.008*\"ice\" + 0.007*\"surface\" + 0.004*\"space\" + 0.004*\"solar\" + 0.004*\"nucleation\"')\n",
      "(6, '0.008*\"urban\" + 0.007*\"surface\" + 0.006*\"change\" + 0.005*\"deposition\" + 0.005*\"land\" + 0.005*\"temperature\"')\n",
      "(7, '0.008*\"cre\" + 0.006*\"forecast\" + 0.006*\"ﬁeld\" + 0.005*\"monsoon\" + 0.005*\"region\" + 0.004*\"debris\"')\n"
     ]
    }
   ],
   "source": [
    "num_passes = 5\n",
    "num_topics = 8\n",
    "words_per_topic = 6\n",
    "\n",
    "dictionary = corpora.Dictionary(document_list)\n",
    "corpus = [dictionary.doc2bow(text) for text in document_list]\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, passes=num_passes)\n",
    "topics = lda_model.print_topics(num_topics=num_topics, num_words=words_per_topic)\n",
    "# Now let's print the topics found\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
