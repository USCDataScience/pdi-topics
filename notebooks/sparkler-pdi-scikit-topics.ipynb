{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "## Online \n",
    "solr_root = 'http://docker:8983/solr/crawldb/select?fl=url,extracted_text,content-type_t_hd&'\n",
    "solr_query = 'q=rows=200&start=1&wt=json'\n",
    "solr_url = solr_root + solr_query\n",
    "req = urllib.request.Request(solr_url)\n",
    "# parsing response\n",
    "r = urllib.request.urlopen(req).read()\n",
    "json_response = json.loads(r.decode('utf-8'))\n",
    "solr_documents = json_response['response']['docs']\n",
    "\n",
    "print(\"Processing {0} documents. \\n\".format(len(solr_documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tokenization & Stop Words & Wordnet Lemmatizer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "## we need stemer\n",
    "stemmer = WordNetLemmatizer()\n",
    "## our custom stop words\n",
    "my_stop_words = {\n",
    "                    'http', 'www', 'edu', 'org', 'com', 'rda', 'data', 'researcher', 'event', 'service',\n",
    "                    'group', 'research', 'community', 'use', 'work', 'member', 'case', 'science',\n",
    "                    'meeting', 'organisational', 'news', 'plenary', 'recommendation', 'project', 'standard',\n",
    "                    'statement', 'school', 'university', 'membership', 'output', '2017', 'brokering',\n",
    "                    'stakeholder', 'repository', 'user', 'citation', 'chair', 'framework', 'information',\n",
    "                    'metadata', 'content', 'sharing', 'pid', 'type', 'record'\n",
    "                }\n",
    "stop_words = my_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "# document list will contain our corpus after cleaning it.\n",
    "document_list = []\n",
    "# pairs is a list of the urls and the size of their content\n",
    "pairs = []\n",
    "# just the documents urls\n",
    "urls = []\n",
    "\n",
    "# There must be an error with the stop words or the scikit algos are doing something off, pid should not be present\n",
    "# in the corpus and yet it shows up as a topic.\n",
    "\n",
    "for item in solr_documents:\n",
    "    # If we apply NER it should be the first step.\n",
    "    # We tokenize words and lower case them(for now)\n",
    "    #tokens = tokenizer.tokenize((item['content'][0]))\n",
    "    tokens = tokenizer.tokenize((item['extracted_text']))\n",
    "    # We lematize (stemming)\n",
    "    stemmed_tokens = [stemmer.lemmatize(i) for i in tokens]\n",
    "    # If the token is not in our stop words and the length is >2 and <20 we add it to the cleaned document\n",
    "    document = [i for i in stemmed_tokens if i not in stop_words and (len(i) > 2 and len(i) < 25)]\n",
    "    # To debug uncomment the next line\n",
    "    #print(\"{0}\\n Document size before stop words: {1}, after: {2} \".format(item['url'],len(stemmed_tokens),len(document)))\n",
    "    document_list.append(document)\n",
    "    pairs.append((item['url'],len(document)))\n",
    "    urls.append(item['url'])\n",
    "    \n",
    "# Aux print function for topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \" + \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])) \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Run NMF and LDA on the corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_features = 1000 # Number of representative tokens \n",
    "no_topics = 10 # can be changed it according to data and need\n",
    "minimum_likelihood = 0.10 # If a topic above this % is contained in a document it will be considered as present\n",
    "\n",
    "documents = [' '.join(doc) for doc in document_list]\n",
    "\n",
    "# Make sure to have enough data on Solr index in ordrer to avoid Invalid Index error \n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# Run NMF\n",
    "nmf_components = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_documents = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit_transform(tfidf)\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# Run LDA\n",
    "lda_components = LatentDirichletAllocation(n_components=no_topics, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_documents = LatentDirichletAllocation(n_components=no_topics, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit_transform(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Building Topic Document Matrix for LDA and NMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_matrix = []\n",
    "lda_matrix = []\n",
    "\n",
    "lda_topic_documents = [[] for i in range(no_topics)]\n",
    "nmf_topic_documents = [[] for i in range(no_topics)]\n",
    "\n",
    "for idx, doc in enumerate(lda_documents):\n",
    "    doc_topics = {'url': urls[idx],\n",
    "                       'topics' :[(float(\"{0:.4f}\".format(doc[i])),i)\n",
    "                                  for i in doc.argsort() if doc[i]> minimum_likelihood]}\n",
    "    for topic in doc_topics['topics']:\n",
    "         lda_topic_documents[topic[1]].append((urls[idx],topic[0]))\n",
    "    lda_matrix.append(doc_topics)\n",
    "\n",
    "for idx, doc in enumerate(nmf_documents):\n",
    "    doc_topics = {'url': urls[idx],\n",
    "                       'topics' :[(float(\"{0:.4f}\".format(doc[i])),i)\n",
    "                                  for i in doc.argsort() if doc[i]> minimum_likelihood]}\n",
    "    \n",
    "    for topic in doc_topics['topics']:\n",
    "         nmf_topic_documents[topic[1]].append((urls[idx],topic[0]))\n",
    "    nmf_matrix.append(doc_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 matrices for each method. For LDA `lda_topic_documents` contains a list of all the topics and each row has the documents that the model predicted they belonged to. The second matrix `lda_matrix` is the inverse, a list of the documents and the topics each document talks about with their probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the topics created by NMF\n",
    "print(\"\\nNMF TOPICS\\n \")\n",
    "display_topics(nmf_components, tfidf_feature_names, no_top_words=8)\n",
    "\n",
    "# Now we'll print the topics created by LDA\n",
    "print(\"\\nLDA TOPICS\\n \")\n",
    "display_topics(nmf_components, tf_feature_names, no_top_words=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We can also see what the models predicted for each topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first topic and their documents:\n",
    "lda_topic_index = 1\n",
    "nmf_topic_index = 9\n",
    "\n",
    "# can be changed it according to data and need, cannot exceed the number of docs processed\n",
    "document_index = 1\n",
    "\n",
    "def getkey(doc):\n",
    "    return doc[1]\n",
    "\n",
    "print(\"NMF Topic {} Contains: \\n\".format(nmf_topic_index))\n",
    "for doc in sorted(nmf_topic_documents[nmf_topic_index],key=getkey):\n",
    "    print (doc)\n",
    "\n",
    "# Print a document and their topics:\n",
    "print(\"\\nDocument: {0}\".format(urls[document_index]))\n",
    "print(\" Topics According to NMF: {0}\".format(nmf_matrix[document_index]['topics']))\n",
    "print(\" Topics According to LDA: {0}\".format(lda_matrix[document_index]['topics']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
